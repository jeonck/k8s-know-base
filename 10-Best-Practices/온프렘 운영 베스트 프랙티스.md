# ì˜¨í”„ë ˜ ìš´ì˜ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

## ğŸ¯ ê°œìš”

ì˜¨í”„ë ˜ ì¿ ë²„ë„¤í‹°ìŠ¤ í™˜ê²½ì—ì„œ ì•ˆì •ì ì´ê³  íš¨ìœ¨ì ì¸ ìš´ì˜ì„ ìœ„í•œ ê²€ì¦ëœ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ì‹¤ì œ ìš´ì˜ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ì‹¤ì „ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ—ï¸ ì¸í”„ë¼ ì„¤ê³„ ì›ì¹™

### ê³ ê°€ìš©ì„± ì„¤ê³„

```yaml
# ê³ ê°€ìš©ì„± êµ¬ì„± ì˜ˆì‹œ
Control Plane:
  - ë§ˆìŠ¤í„° ë…¸ë“œ: 3ëŒ€ (í™€ìˆ˜ ê°œ)
  - etcd: ë³„ë„ í´ëŸ¬ìŠ¤í„° ë˜ëŠ” ë§ˆìŠ¤í„°ì™€ ë™ì¼
  - ë¡œë“œë°¸ëŸ°ì„œ: 2ëŒ€ ì´ìƒ (HA êµ¬ì„±)

Worker Nodes:
  - ìµœì†Œ 3ëŒ€ ì´ìƒ
  - ë‹¤ì¤‘ ê°€ìš© ì˜ì—­ ë¶„ì‚°
  - ìë™ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜

Storage:
  - ë¶„ì‚° ìŠ¤í† ë¦¬ì§€ (Ceph, GlusterFS)
  - ë°±ì—… ë° ë³µì œ ì •ì±…
  - ìŠ¤ëƒ…ìƒ· ìë™í™”
```

### ë¦¬ì†ŒìŠ¤ ê³„íš

```yaml
# ë¦¬ì†ŒìŠ¤ í• ë‹¹ ì „ëµ
CPU í• ë‹¹:
  System Reserved: 10-20%
  Kubelet Reserved: 5-10%
  Application: 70-85%

Memory í• ë‹¹:
  System Reserved: 2-4GB
  Kubelet Reserved: 1-2GB
  Application: ë‚˜ë¨¸ì§€

Storage ê³„íš:
  OS: 50-100GB SSD
  Container Images: 100-200GB SSD
  Data: ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ (HDD/SSD)
  Backup: ë°ì´í„°ì˜ 3ë°° ì´ìƒ
```

## ğŸ› ï¸ í´ëŸ¬ìŠ¤í„° ê´€ë¦¬

### ë…¸ë“œ ë ˆì´ë¸”ë§ ì „ëµ

```bash
# í•˜ë“œì›¨ì–´ ê¸°ë°˜ ë ˆì´ë¸”ë§
kubectl label node worker-01 hardware.company.com/cpu=high-performance
kubectl label node worker-01 hardware.company.com/memory=high-memory
kubectl label node worker-01 hardware.company.com/storage=ssd
kubectl label node worker-01 hardware.company.com/network=10gbps

# í™˜ê²½ë³„ ë ˆì´ë¸”ë§
kubectl label node worker-01 environment=production
kubectl label node worker-02 environment=staging
kubectl label node worker-03 environment=development

# ìš©ë„ë³„ ë ˆì´ë¸”ë§
kubectl label node worker-01 workload.company.com/type=compute
kubectl label node worker-02 workload.company.com/type=database
kubectl label node worker-03 workload.company.com/type=monitoring

# ì§€ì—­ë³„ ë ˆì´ë¸”ë§
kubectl label node worker-01 topology.kubernetes.io/zone=zone-a
kubectl label node worker-02 topology.kubernetes.io/zone=zone-b
kubectl label node worker-03 topology.kubernetes.io/zone=zone-c
```

### Taintsì™€ Tolerations í™œìš©

```yaml
# taints-tolerations.yaml
---
# ìš´ì˜ í™˜ê²½ ì „ìš© ë…¸ë“œ
apiVersion: v1
kind: Node
metadata:
  name: prod-worker-01
spec:
  taints:
  - key: environment
    value: production
    effect: NoSchedule
  - key: workload-type
    value: database
    effect: NoExecute

---
# ë°ì´í„°ë² ì´ìŠ¤ Podì— Toleration ì ìš©
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-cluster
spec:
  template:
    spec:
      tolerations:
      - key: environment
        operator: Equal
        value: production
        effect: NoSchedule
      - key: workload-type
        operator: Equal
        value: database
        effect: NoExecute
      nodeSelector:
        workload.company.com/type: database
        environment: production
```

### ë¦¬ì†ŒìŠ¤ ì¿¼í„° ê´€ë¦¬

```yaml
# resource-quotas.yaml
---
# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ ë¦¬ì†ŒìŠ¤ ì œí•œ
apiVersion: v1
kind: ResourceQuota
metadata:
  name: production-quota
  namespace: production
spec:
  hard:
    # ì»´í“¨íŠ¸ ë¦¬ì†ŒìŠ¤
    requests.cpu: "20"
    requests.memory: 40Gi
    limits.cpu: "40"
    limits.memory: 80Gi
    
    # ìŠ¤í† ë¦¬ì§€ ë¦¬ì†ŒìŠ¤
    requests.storage: 500Gi
    persistentvolumeclaims: "20"
    
    # ì˜¤ë¸Œì íŠ¸ ìˆ˜ ì œí•œ
    pods: "50"
    services: "10"
    secrets: "20"
    configmaps: "20"
    
    # LoadBalancer ì„œë¹„ìŠ¤ ì œí•œ
    services.loadbalancers: "2"
    services.nodeports: "5"

---
# ê°œë°œ í™˜ê²½ ì œí•œ
apiVersion: v1
kind: ResourceQuota
metadata:
  name: development-quota
  namespace: development
spec:
  hard:
    requests.cpu: "5"
    requests.memory: 10Gi
    limits.cpu: "10"
    limits.memory: 20Gi
    pods: "20"
```

### LimitRange ì„¤ì •

```yaml
# limit-ranges.yaml
---
# ê¸°ë³¸ ë¦¬ì†ŒìŠ¤ ì œí•œ
apiVersion: v1
kind: LimitRange
metadata:
  name: default-limits
  namespace: production
spec:
  limits:
  # Pod ë ˆë²¨ ì œí•œ
  - type: Pod
    max:
      cpu: "4"
      memory: 8Gi
    min:
      cpu: 10m
      memory: 10Mi
  
  # ì»¨í…Œì´ë„ˆ ë ˆë²¨ ì œí•œ
  - type: Container
    default:
      cpu: 100m
      memory: 128Mi
    defaultRequest:
      cpu: 50m
      memory: 64Mi
    max:
      cpu: "2"
      memory: 4Gi
    min:
      cpu: 10m
      memory: 10Mi
  
  # PVC ì œí•œ
  - type: PersistentVolumeClaim
    max:
      storage: 100Gi
    min:
      storage: 1Gi
```

## ğŸ”§ ìš´ì˜ ìë™í™”

### ì •ê¸° ìœ ì§€ë³´ìˆ˜ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# cluster-maintenance.sh

# ì¿ ë²„ë„¤í‹°ìŠ¤ í´ëŸ¬ìŠ¤í„° ì •ê¸° ìœ ì§€ë³´ìˆ˜ ìŠ¤í¬ë¦½íŠ¸

LOG_FILE="/var/log/k8s-maintenance.log"
DATE=$(date '+%Y-%m-%d %H:%M:%S')

log() {
    echo "[$DATE] $1" | tee -a $LOG_FILE
}

# 1. í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
check_cluster_health() {
    log "=== Cluster Health Check ==="
    
    # API ì„œë²„ ìƒíƒœ
    if kubectl cluster-info > /dev/null 2>&1; then
        log "âœ“ API Server is healthy"
    else
        log "âœ— API Server is not responding"
        return 1
    fi
    
    # ë…¸ë“œ ìƒíƒœ í™•ì¸
    NOT_READY=$(kubectl get nodes --no-headers | grep -v Ready | wc -l)
    if [ $NOT_READY -eq 0 ]; then
        log "âœ“ All nodes are Ready"
    else
        log "âœ— $NOT_READY nodes are not Ready"
        kubectl get nodes --no-headers | grep -v Ready
    fi
    
    # ì‹œìŠ¤í…œ Pod ìƒíƒœ
    FAILED_PODS=$(kubectl get pods -n kube-system --no-headers | grep -v Running | grep -v Completed | wc -l)
    if [ $FAILED_PODS -eq 0 ]; then
        log "âœ“ All system pods are running"
    else
        log "âœ— $FAILED_PODS system pods are not running"
        kubectl get pods -n kube-system --no-headers | grep -v Running | grep -v Completed
    fi
}

# 2. ë¦¬ì†ŒìŠ¤ ì •ë¦¬
cleanup_resources() {
    log "=== Resource Cleanup ==="
    
    # ì™„ë£Œëœ Job ì •ë¦¬ (7ì¼ ì´ìƒ)
    kubectl get jobs --all-namespaces -o json | jq -r '.items[] | select(.status.completionTime != null) | select((now - (.status.completionTime | strptime("%Y-%m-%dT%H:%M:%SZ") | mktime)) > (7*24*3600)) | "\(.metadata.namespace) \(.metadata.name)"' | while read ns job; do
        log "Cleaning up completed job: $ns/$job"
        kubectl delete job $job -n $ns
    done
    
    # ì‹¤íŒ¨í•œ Pod ì •ë¦¬
    kubectl get pods --all-namespaces --field-selector=status.phase=Failed -o json | jq -r '.items[] | "\(.metadata.namespace) \(.metadata.name)"' | while read ns pod; do
        log "Cleaning up failed pod: $ns/$pod"
        kubectl delete pod $pod -n $ns
    done
    
    # Evicted Pod ì •ë¦¬
    kubectl get pods --all-namespaces --field-selector=status.phase=Failed -o json | jq -r '.items[] | select(.status.reason == "Evicted") | "\(.metadata.namespace) \(.metadata.name)"' | while read ns pod; do
        log "Cleaning up evicted pod: $ns/$pod"
        kubectl delete pod $pod -n $ns
    done
}

# 3. ë©”íŠ¸ë¦­ ìˆ˜ì§‘
collect_metrics() {
    log "=== Metrics Collection ==="
    
    # ë…¸ë“œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
    log "Node Resource Usage:"
    kubectl top nodes
    
    # Pod ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ (ìƒìœ„ 10ê°œ)
    log "Top 10 CPU consuming pods:"
    kubectl top pods --all-namespaces --sort-by=cpu | head -10
    
    log "Top 10 Memory consuming pods:"
    kubectl top pods --all-namespaces --sort-by=memory | head -10
    
    # ìŠ¤í† ë¦¬ì§€ ì‚¬ìš©ëŸ‰
    log "Storage Usage:"
    kubectl get pv -o custom-columns=NAME:.metadata.name,CAPACITY:.spec.capacity.storage,STATUS:.status.phase,CLAIM:.spec.claimRef.name
}

# 4. ë°±ì—… í™•ì¸
check_backups() {
    log "=== Backup Status Check ==="
    
    # etcd ë°±ì—… í™•ì¸
    BACKUP_DIR="/var/backups/etcd"
    LATEST_BACKUP=$(ls -t $BACKUP_DIR/etcd-snapshot-*.db 2>/dev/null | head -1)
    
    if [ -n "$LATEST_BACKUP" ]; then
        BACKUP_AGE=$(stat -c %Y "$LATEST_BACKUP")
        CURRENT_TIME=$(date +%s)
        AGE_HOURS=$(( ($CURRENT_TIME - $BACKUP_AGE) / 3600 ))
        
        if [ $AGE_HOURS -lt 24 ]; then
            log "âœ“ Latest etcd backup is $AGE_HOURS hours old"
        else
            log "âœ— Latest etcd backup is $AGE_HOURS hours old (>24h)"
        fi
    else
        log "âœ— No etcd backups found"
    fi
}

# 5. ì¸ì¦ì„œ ë§Œë£Œ í™•ì¸
check_certificates() {
    log "=== Certificate Expiration Check ==="
    
    kubeadm certs check-expiration 2>/dev/null | while read line; do
        if echo "$line" | grep -q "CERTIFICATE"; then
            log "$line"
        fi
        if echo "$line" | grep -q "days"; then
            DAYS=$(echo "$line" | grep -o '[0-9]*days' | grep -o '[0-9]*')
            if [ "$DAYS" -lt 30 ]; then
                log "âš  Certificate expires in $DAYS days: $line"
            fi
        fi
    done
}

# ë©”ì¸ ì‹¤í–‰
main() {
    log "Starting cluster maintenance routine"
    
    check_cluster_health
    cleanup_resources
    collect_metrics
    check_backups
    check_certificates
    
    log "Cluster maintenance completed"
}

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
main "$@"
```

### ë°±ì—… ìë™í™”

```bash
#!/bin/bash
# backup-automation.sh

# etcd ë°±ì—… ìŠ¤í¬ë¦½íŠ¸
backup_etcd() {
    BACKUP_DIR="/var/backups/etcd"
    DATE=$(date +%Y%m%d_%H%M%S)
    BACKUP_FILE="$BACKUP_DIR/etcd-snapshot-$DATE.db"
    
    mkdir -p $BACKUP_DIR
    
    # etcd ë°±ì—… ì‹¤í–‰
    ETCDCTL_API=3 etcdctl snapshot save $BACKUP_FILE \
        --endpoints=https://127.0.0.1:2379 \
        --cacert=/etc/kubernetes/pki/etcd/ca.crt \
        --cert=/etc/kubernetes/pki/etcd/server.crt \
        --key=/etc/kubernetes/pki/etcd/server.key
    
    # ë°±ì—… ê²€ì¦
    ETCDCTL_API=3 etcdctl snapshot status $BACKUP_FILE
    
    # ì••ì¶•
    gzip $BACKUP_FILE
    
    # ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ (30ì¼ ì´ìƒ)
    find $BACKUP_DIR -name "etcd-snapshot-*.db.gz" -mtime +30 -delete
    
    echo "etcd backup completed: $BACKUP_FILE.gz"
}

# ì„¤ì • íŒŒì¼ ë°±ì—…
backup_configs() {
    BACKUP_DIR="/var/backups/kubernetes-configs"
    DATE=$(date +%Y%m%d_%H%M%S)
    
    mkdir -p $BACKUP_DIR
    
    # ì¿ ë²„ë„¤í‹°ìŠ¤ ì„¤ì • ë°±ì—…
    tar -czf "$BACKUP_DIR/k8s-configs-$DATE.tar.gz" \
        /etc/kubernetes \
        /var/lib/kubelet/config.yaml \
        /etc/systemd/system/kubelet.service.d
    
    # YAML ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ë°±ì—…
    kubectl get all --all-namespaces -o yaml > "$BACKUP_DIR/all-resources-$DATE.yaml"
    
    # ì‹œí¬ë¦¿ ë°±ì—… (base64 ì¸ì½”ë”©ë¨)
    kubectl get secrets --all-namespaces -o yaml > "$BACKUP_DIR/secrets-$DATE.yaml"
    
    # ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ
    find $BACKUP_DIR -name "*.tar.gz" -mtime +7 -delete
    find $BACKUP_DIR -name "*.yaml" -mtime +7 -delete
    
    echo "Config backup completed"
}

# ì›ê²© ë°±ì—… (ì„ íƒì‚¬í•­)
sync_to_remote() {
    REMOTE_HOST="backup.company.local"
    REMOTE_PATH="/backups/kubernetes"
    
    # rsyncë¥¼ ì‚¬ìš©í•œ ì›ê²© ë™ê¸°í™”
    rsync -avz --delete /var/backups/ $REMOTE_HOST:$REMOTE_PATH/
    
    echo "Remote sync completed"
}

# ë©”ì¸ ì‹¤í–‰
backup_etcd
backup_configs
# sync_to_remote  # í•„ìš”ì‹œ í™œì„±í™”
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§ê³¼ ì•Œë¦¼

### í•µì‹¬ ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§

```yaml
# monitoring-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: onprem-monitoring-rules
  namespace: monitoring
spec:
  groups:
  - name: kubernetes-cluster
    interval: 30s
    rules:
    # ë…¸ë“œ ìƒíƒœ ëª¨ë‹ˆí„°ë§
    - alert: NodeDown
      expr: up{job="node-exporter"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Node {{ $labels.instance }} is down"
        
    # ë””ìŠ¤í¬ ê³µê°„ ëª¨ë‹ˆí„°ë§
    - alert: DiskSpaceLow
      expr: (node_filesystem_avail_bytes{fstype!="tmpfs"} / node_filesystem_size_bytes{fstype!="tmpfs"}) * 100 < 15
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Low disk space on {{ $labels.instance }}"
        
    # etcd ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    - alert: EtcdHighCommitDurations
      expr: histogram_quantile(0.99, sum(rate(etcd_disk_backend_commit_duration_seconds_bucket[5m])) by (instance, le)) > 0.25
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "etcd commit durations are high"
        
    # API ì„œë²„ ì‘ë‹µ ì‹œê°„
    - alert: APIServerHighLatency
      expr: histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{subresource!="log",verb!~"^(?:CONNECT|WATCH)$"}[5m])) without(instance, pod)) > 1
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "API server latency is high"
```

### ë¡œê·¸ ê´€ë¦¬ ì „ëµ

```yaml
# logging-strategy.yaml
# ì¤‘ì•™ ì§‘ì¤‘ì‹ ë¡œê¹… ì„¤ì •

# Fluentd DaemonSet
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  namespace: kube-system
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule
      containers:
      - name: fluentd-elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v3.4.0
        env:
        - name: FLUENTD_SYSTEMD_CONF
          value: disable
        - name: FLUENTD_PROMETHEUS_CONF
          value: disable
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: config-volume
          mountPath: /etc/fluent/config.d
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: config-volume
        configMap:
          name: fluentd-config
```

## ğŸ”’ ë³´ì•ˆ ê°•í™”

### Pod Security Standards

```yaml
# pod-security-standards.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    # ì—„ê²©í•œ ë³´ì•ˆ ì •ì±…
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
---
apiVersion: v1
kind: Namespace
metadata:
  name: monitoring
  labels:
    # ëª¨ë‹ˆí„°ë§ì€ ì¼ë¶€ ê¶Œí•œ í•„ìš”
    pod-security.kubernetes.io/enforce: baseline
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
```

### ë„¤íŠ¸ì›Œí¬ ì •ì±…

```yaml
# network-policies.yaml
---
# ê¸°ë³¸ ê±°ë¶€ ì •ì±…
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress

---
# ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ì •ì±…
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: web-app-policy
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: web-app
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 8080
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: database
    ports:
    - protocol: TCP
      port: 3306
  # DNS í—ˆìš©
  - to: []
    ports:
    - protocol: UDP
      port: 53
```

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

### kubelet íŠœë‹

```yaml
# kubelet-config.yaml
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
# ì´ë¯¸ì§€ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
# ì»¨í…Œì´ë„ˆ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
containerGCThreshold:
  minAge: 0s
  maxPerPodContainer: 1
  maxContainers: -1
# ì´ë²¤íŠ¸ ê´€ë¦¬
eventRecordQPS: 50
eventBurst: 100
# ë¦¬ì†ŒìŠ¤ ì˜ˆì•½
systemReserved:
  cpu: "0.5"
  memory: "1Gi"
  ephemeral-storage: "10Gi"
kubeReserved:
  cpu: "0.5"
  memory: "1Gi"
  ephemeral-storage: "10Gi"
# ë©”ëª¨ë¦¬ ê´€ë¦¬
memorySwap: {}
memoryThrottlingFactor: 0.8
# ë³‘ë ¬ ì²˜ë¦¬
maxPods: 250
podPidsLimit: 4096
# ë¡œê·¸ ê´€ë¦¬
containerLogMaxSize: 100Mi
containerLogMaxFiles: 5
```

### API ì„œë²„ íŠœë‹

```yaml
# api-server-optimization.yaml
# /etc/kubernetes/manifests/kube-apiserver.yaml ìˆ˜ì • ë‚´ìš©

spec:
  containers:
  - command:
    - kube-apiserver
    # ê°ì‚¬ ë¡œê·¸ ìµœì í™”
    - --audit-log-maxage=30
    - --audit-log-maxbackup=3
    - --audit-log-maxsize=100
    # ìš”ì²­ ì œí•œ
    - --max-requests-inflight=400
    - --max-mutating-requests-inflight=200
    # ì›Œì¹˜ ìºì‹œ ìµœì í™”
    - --watch-cache-sizes=nodes#100,pods#1000,services#100
    # etcd ì—°ê²° ìµœì í™”
    - --etcd-compaction-interval=300s
    # API ìš°ì„ ìˆœìœ„ ë° ê³µì •ì„±
    - --enable-priority-and-fairness=true
    - --request-timeout=60s
```

## ğŸš¨ ì¥ì•  ëŒ€ì‘ ì ˆì°¨

### ì¥ì•  ëŒ€ì‘ ì²´í¬ë¦¬ìŠ¤íŠ¸

```markdown
# ì¥ì•  ëŒ€ì‘ ì²´í¬ë¦¬ìŠ¤íŠ¸

## 1ë‹¨ê³„: ì´ˆê¸° ëŒ€ì‘ (5ë¶„ ì´ë‚´)
- [ ] ì¥ì•  ë²”ìœ„ í™•ì¸
- [ ] í´ëŸ¬ìŠ¤í„° API ì‘ë‹µ í™•ì¸
- [ ] ì£¼ìš” ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
- [ ] ëª¨ë‹ˆí„°ë§ ì•Œë¦¼ í™•ì¸

## 2ë‹¨ê³„: ìƒì„¸ ì§„ë‹¨ (15ë¶„ ì´ë‚´)
- [ ] ë…¸ë“œ ìƒíƒœ í™•ì¸
- [ ] ì‹œìŠ¤í…œ Pod ìƒíƒœ í™•ì¸
- [ ] ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í™•ì¸
- [ ] ì´ë²¤íŠ¸ ë¡œê·¸ ë¶„ì„

## 3ë‹¨ê³„: ë³µêµ¬ ì‘ì—…
- [ ] ì˜í–¥ë°›ì€ ì»´í¬ë„ŒíŠ¸ ì¬ì‹œì‘
- [ ] íŠ¸ë˜í”½ ìš°íšŒ (í•„ìš”ì‹œ)
- [ ] ë°±ì—…ì—ì„œ ë³µì› (í•„ìš”ì‹œ)
- [ ] ì„œë¹„ìŠ¤ ê²€ì¦

## 4ë‹¨ê³„: ì‚¬í›„ ì¡°ì¹˜
- [ ] ê·¼ë³¸ ì›ì¸ ë¶„ì„
- [ ] ì¬ë°œ ë°©ì§€ ëŒ€ì±… ìˆ˜ë¦½
- [ ] ë¬¸ì„œ ì—…ë°ì´íŠ¸
- [ ] íŒ€ ë‚´ ê³µìœ 
```

### ìë™ ë³µêµ¬ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# auto-recovery.sh

# ìë™ ë³µêµ¬ ìŠ¤í¬ë¦½íŠ¸ (ì‹ ì¤‘í•˜ê²Œ ì‚¬ìš©)

recover_node() {
    local node=$1
    
    echo "Attempting to recover node: $node"
    
    # ë…¸ë“œ ë“œë ˆì¸
    kubectl drain $node --ignore-daemonsets --delete-emptydir-data --force
    
    # SSHë¡œ ë…¸ë“œì—ì„œ kubelet ì¬ì‹œì‘
    ssh $node "systemctl restart kubelet containerd"
    
    # ë…¸ë“œ ì–¸ì½”ë“ 
    kubectl uncordon $node
    
    echo "Node recovery attempted for: $node"
}

recover_system_pods() {
    echo "Checking system pods..."
    
    # ì‹¤íŒ¨í•œ ì‹œìŠ¤í…œ Pod ì¬ì‹œì‘
    kubectl get pods -n kube-system --field-selector=status.phase=Failed -o name | while read pod; do
        echo "Restarting failed system pod: $pod"
        kubectl delete $pod -n kube-system
    done
}

# ì‚¬ìš© ì˜ˆ: ./auto-recovery.sh node worker-01
if [ "$1" = "node" ] && [ -n "$2" ]; then
    recover_node $2
elif [ "$1" = "pods" ]; then
    recover_system_pods
else
    echo "Usage: $0 {node <node-name>|pods}"
fi
```

---

> ğŸ’¡ **ì‹¤ì „ ê²½í—˜**: ì˜¨í”„ë ˜ ìš´ì˜ì˜ í•µì‹¬ì€ ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë¬¸ì œì— ëŒ€í•œ ì² ì €í•œ ì¤€ë¹„ì…ë‹ˆë‹¤. ìë™í™”ëŠ” ì ì§„ì ìœ¼ë¡œ ë„ì…í•˜ê³ , í•­ìƒ ìˆ˜ë™ ë³µêµ¬ ì ˆì°¨ë¥¼ ë³‘í–‰í•˜ì„¸ìš”. ë°±ì—…ê³¼ ëª¨ë‹ˆí„°ë§ì€ ì ˆëŒ€ ì†Œí™€íˆ í•˜ì§€ ë§ˆì„¸ìš”.

íƒœê·¸: #bestpractices #operations #automation #onprem #production