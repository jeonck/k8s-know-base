# 온프렘 운영 베스트 프랙티스

## 🎯 개요

온프렘 쿠버네티스 환경에서 안정적이고 효율적인 운영을 위한 검증된 베스트 프랙티스를 제시합니다. 실제 운영 경험을 바탕으로 한 실전 가이드입니다.

## 🏗️ 인프라 설계 원칙

### 고가용성 설계

```yaml
# 고가용성 구성 예시
Control Plane:
  - 마스터 노드: 3대 (홀수 개)
  - etcd: 별도 클러스터 또는 마스터와 동일
  - 로드밸런서: 2대 이상 (HA 구성)

Worker Nodes:
  - 최소 3대 이상
  - 다중 가용 영역 분산
  - 자동 복구 메커니즘

Storage:
  - 분산 스토리지 (Ceph, GlusterFS)
  - 백업 및 복제 정책
  - 스냅샷 자동화
```

### 리소스 계획

```yaml
# 리소스 할당 전략
CPU 할당:
  System Reserved: 10-20%
  Kubelet Reserved: 5-10%
  Application: 70-85%

Memory 할당:
  System Reserved: 2-4GB
  Kubelet Reserved: 1-2GB
  Application: 나머지

Storage 계획:
  OS: 50-100GB SSD
  Container Images: 100-200GB SSD
  Data: 요구사항에 따라 (HDD/SSD)
  Backup: 데이터의 3배 이상
```

## 🛠️ 클러스터 관리

### 노드 레이블링 전략

```bash
# 하드웨어 기반 레이블링
kubectl label node worker-01 hardware.company.com/cpu=high-performance
kubectl label node worker-01 hardware.company.com/memory=high-memory
kubectl label node worker-01 hardware.company.com/storage=ssd
kubectl label node worker-01 hardware.company.com/network=10gbps

# 환경별 레이블링
kubectl label node worker-01 environment=production
kubectl label node worker-02 environment=staging
kubectl label node worker-03 environment=development

# 용도별 레이블링
kubectl label node worker-01 workload.company.com/type=compute
kubectl label node worker-02 workload.company.com/type=database
kubectl label node worker-03 workload.company.com/type=monitoring

# 지역별 레이블링
kubectl label node worker-01 topology.kubernetes.io/zone=zone-a
kubectl label node worker-02 topology.kubernetes.io/zone=zone-b
kubectl label node worker-03 topology.kubernetes.io/zone=zone-c
```

### Taints와 Tolerations 활용

```yaml
# taints-tolerations.yaml
---
# 운영 환경 전용 노드
apiVersion: v1
kind: Node
metadata:
  name: prod-worker-01
spec:
  taints:
  - key: environment
    value: production
    effect: NoSchedule
  - key: workload-type
    value: database
    effect: NoExecute

---
# 데이터베이스 Pod에 Toleration 적용
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-cluster
spec:
  template:
    spec:
      tolerations:
      - key: environment
        operator: Equal
        value: production
        effect: NoSchedule
      - key: workload-type
        operator: Equal
        value: database
        effect: NoExecute
      nodeSelector:
        workload.company.com/type: database
        environment: production
```

### 리소스 쿼터 관리

```yaml
# resource-quotas.yaml
---
# 네임스페이스별 리소스 제한
apiVersion: v1
kind: ResourceQuota
metadata:
  name: production-quota
  namespace: production
spec:
  hard:
    # 컴퓨트 리소스
    requests.cpu: "20"
    requests.memory: 40Gi
    limits.cpu: "40"
    limits.memory: 80Gi
    
    # 스토리지 리소스
    requests.storage: 500Gi
    persistentvolumeclaims: "20"
    
    # 오브젝트 수 제한
    pods: "50"
    services: "10"
    secrets: "20"
    configmaps: "20"
    
    # LoadBalancer 서비스 제한
    services.loadbalancers: "2"
    services.nodeports: "5"

---
# 개발 환경 제한
apiVersion: v1
kind: ResourceQuota
metadata:
  name: development-quota
  namespace: development
spec:
  hard:
    requests.cpu: "5"
    requests.memory: 10Gi
    limits.cpu: "10"
    limits.memory: 20Gi
    pods: "20"
```

### LimitRange 설정

```yaml
# limit-ranges.yaml
---
# 기본 리소스 제한
apiVersion: v1
kind: LimitRange
metadata:
  name: default-limits
  namespace: production
spec:
  limits:
  # Pod 레벨 제한
  - type: Pod
    max:
      cpu: "4"
      memory: 8Gi
    min:
      cpu: 10m
      memory: 10Mi
  
  # 컨테이너 레벨 제한
  - type: Container
    default:
      cpu: 100m
      memory: 128Mi
    defaultRequest:
      cpu: 50m
      memory: 64Mi
    max:
      cpu: "2"
      memory: 4Gi
    min:
      cpu: 10m
      memory: 10Mi
  
  # PVC 제한
  - type: PersistentVolumeClaim
    max:
      storage: 100Gi
    min:
      storage: 1Gi
```

## 🔧 운영 자동화

### 정기 유지보수 스크립트

```bash
#!/bin/bash
# cluster-maintenance.sh

# 쿠버네티스 클러스터 정기 유지보수 스크립트

LOG_FILE="/var/log/k8s-maintenance.log"
DATE=$(date '+%Y-%m-%d %H:%M:%S')

log() {
    echo "[$DATE] $1" | tee -a $LOG_FILE
}

# 1. 클러스터 상태 확인
check_cluster_health() {
    log "=== Cluster Health Check ==="
    
    # API 서버 상태
    if kubectl cluster-info > /dev/null 2>&1; then
        log "✓ API Server is healthy"
    else
        log "✗ API Server is not responding"
        return 1
    fi
    
    # 노드 상태 확인
    NOT_READY=$(kubectl get nodes --no-headers | grep -v Ready | wc -l)
    if [ $NOT_READY -eq 0 ]; then
        log "✓ All nodes are Ready"
    else
        log "✗ $NOT_READY nodes are not Ready"
        kubectl get nodes --no-headers | grep -v Ready
    fi
    
    # 시스템 Pod 상태
    FAILED_PODS=$(kubectl get pods -n kube-system --no-headers | grep -v Running | grep -v Completed | wc -l)
    if [ $FAILED_PODS -eq 0 ]; then
        log "✓ All system pods are running"
    else
        log "✗ $FAILED_PODS system pods are not running"
        kubectl get pods -n kube-system --no-headers | grep -v Running | grep -v Completed
    fi
}

# 2. 리소스 정리
cleanup_resources() {
    log "=== Resource Cleanup ==="
    
    # 완료된 Job 정리 (7일 이상)
    kubectl get jobs --all-namespaces -o json | jq -r '.items[] | select(.status.completionTime != null) | select((now - (.status.completionTime | strptime("%Y-%m-%dT%H:%M:%SZ") | mktime)) > (7*24*3600)) | "\(.metadata.namespace) \(.metadata.name)"' | while read ns job; do
        log "Cleaning up completed job: $ns/$job"
        kubectl delete job $job -n $ns
    done
    
    # 실패한 Pod 정리
    kubectl get pods --all-namespaces --field-selector=status.phase=Failed -o json | jq -r '.items[] | "\(.metadata.namespace) \(.metadata.name)"' | while read ns pod; do
        log "Cleaning up failed pod: $ns/$pod"
        kubectl delete pod $pod -n $ns
    done
    
    # Evicted Pod 정리
    kubectl get pods --all-namespaces --field-selector=status.phase=Failed -o json | jq -r '.items[] | select(.status.reason == "Evicted") | "\(.metadata.namespace) \(.metadata.name)"' | while read ns pod; do
        log "Cleaning up evicted pod: $ns/$pod"
        kubectl delete pod $pod -n $ns
    done
}

# 3. 메트릭 수집
collect_metrics() {
    log "=== Metrics Collection ==="
    
    # 노드 리소스 사용량
    log "Node Resource Usage:"
    kubectl top nodes
    
    # Pod 리소스 사용량 (상위 10개)
    log "Top 10 CPU consuming pods:"
    kubectl top pods --all-namespaces --sort-by=cpu | head -10
    
    log "Top 10 Memory consuming pods:"
    kubectl top pods --all-namespaces --sort-by=memory | head -10
    
    # 스토리지 사용량
    log "Storage Usage:"
    kubectl get pv -o custom-columns=NAME:.metadata.name,CAPACITY:.spec.capacity.storage,STATUS:.status.phase,CLAIM:.spec.claimRef.name
}

# 4. 백업 확인
check_backups() {
    log "=== Backup Status Check ==="
    
    # etcd 백업 확인
    BACKUP_DIR="/var/backups/etcd"
    LATEST_BACKUP=$(ls -t $BACKUP_DIR/etcd-snapshot-*.db 2>/dev/null | head -1)
    
    if [ -n "$LATEST_BACKUP" ]; then
        BACKUP_AGE=$(stat -c %Y "$LATEST_BACKUP")
        CURRENT_TIME=$(date +%s)
        AGE_HOURS=$(( ($CURRENT_TIME - $BACKUP_AGE) / 3600 ))
        
        if [ $AGE_HOURS -lt 24 ]; then
            log "✓ Latest etcd backup is $AGE_HOURS hours old"
        else
            log "✗ Latest etcd backup is $AGE_HOURS hours old (>24h)"
        fi
    else
        log "✗ No etcd backups found"
    fi
}

# 5. 인증서 만료 확인
check_certificates() {
    log "=== Certificate Expiration Check ==="
    
    kubeadm certs check-expiration 2>/dev/null | while read line; do
        if echo "$line" | grep -q "CERTIFICATE"; then
            log "$line"
        fi
        if echo "$line" | grep -q "days"; then
            DAYS=$(echo "$line" | grep -o '[0-9]*days' | grep -o '[0-9]*')
            if [ "$DAYS" -lt 30 ]; then
                log "⚠ Certificate expires in $DAYS days: $line"
            fi
        fi
    done
}

# 메인 실행
main() {
    log "Starting cluster maintenance routine"
    
    check_cluster_health
    cleanup_resources
    collect_metrics
    check_backups
    check_certificates
    
    log "Cluster maintenance completed"
}

# 스크립트 실행
main "$@"
```

### 백업 자동화

```bash
#!/bin/bash
# backup-automation.sh

# etcd 백업 스크립트
backup_etcd() {
    BACKUP_DIR="/var/backups/etcd"
    DATE=$(date +%Y%m%d_%H%M%S)
    BACKUP_FILE="$BACKUP_DIR/etcd-snapshot-$DATE.db"
    
    mkdir -p $BACKUP_DIR
    
    # etcd 백업 실행
    ETCDCTL_API=3 etcdctl snapshot save $BACKUP_FILE \
        --endpoints=https://127.0.0.1:2379 \
        --cacert=/etc/kubernetes/pki/etcd/ca.crt \
        --cert=/etc/kubernetes/pki/etcd/server.crt \
        --key=/etc/kubernetes/pki/etcd/server.key
    
    # 백업 검증
    ETCDCTL_API=3 etcdctl snapshot status $BACKUP_FILE
    
    # 압축
    gzip $BACKUP_FILE
    
    # 오래된 백업 삭제 (30일 이상)
    find $BACKUP_DIR -name "etcd-snapshot-*.db.gz" -mtime +30 -delete
    
    echo "etcd backup completed: $BACKUP_FILE.gz"
}

# 설정 파일 백업
backup_configs() {
    BACKUP_DIR="/var/backups/kubernetes-configs"
    DATE=$(date +%Y%m%d_%H%M%S)
    
    mkdir -p $BACKUP_DIR
    
    # 쿠버네티스 설정 백업
    tar -czf "$BACKUP_DIR/k8s-configs-$DATE.tar.gz" \
        /etc/kubernetes \
        /var/lib/kubelet/config.yaml \
        /etc/systemd/system/kubelet.service.d
    
    # YAML 매니페스트 백업
    kubectl get all --all-namespaces -o yaml > "$BACKUP_DIR/all-resources-$DATE.yaml"
    
    # 시크릿 백업 (base64 인코딩됨)
    kubectl get secrets --all-namespaces -o yaml > "$BACKUP_DIR/secrets-$DATE.yaml"
    
    # 오래된 백업 삭제
    find $BACKUP_DIR -name "*.tar.gz" -mtime +7 -delete
    find $BACKUP_DIR -name "*.yaml" -mtime +7 -delete
    
    echo "Config backup completed"
}

# 원격 백업 (선택사항)
sync_to_remote() {
    REMOTE_HOST="backup.company.local"
    REMOTE_PATH="/backups/kubernetes"
    
    # rsync를 사용한 원격 동기화
    rsync -avz --delete /var/backups/ $REMOTE_HOST:$REMOTE_PATH/
    
    echo "Remote sync completed"
}

# 메인 실행
backup_etcd
backup_configs
# sync_to_remote  # 필요시 활성화
```

## 📊 모니터링과 알림

### 핵심 메트릭 모니터링

```yaml
# monitoring-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: onprem-monitoring-rules
  namespace: monitoring
spec:
  groups:
  - name: kubernetes-cluster
    interval: 30s
    rules:
    # 노드 상태 모니터링
    - alert: NodeDown
      expr: up{job="node-exporter"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Node {{ $labels.instance }} is down"
        
    # 디스크 공간 모니터링
    - alert: DiskSpaceLow
      expr: (node_filesystem_avail_bytes{fstype!="tmpfs"} / node_filesystem_size_bytes{fstype!="tmpfs"}) * 100 < 15
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Low disk space on {{ $labels.instance }}"
        
    # etcd 성능 모니터링
    - alert: EtcdHighCommitDurations
      expr: histogram_quantile(0.99, sum(rate(etcd_disk_backend_commit_duration_seconds_bucket[5m])) by (instance, le)) > 0.25
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "etcd commit durations are high"
        
    # API 서버 응답 시간
    - alert: APIServerHighLatency
      expr: histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{subresource!="log",verb!~"^(?:CONNECT|WATCH)$"}[5m])) without(instance, pod)) > 1
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "API server latency is high"
```

### 로그 관리 전략

```yaml
# logging-strategy.yaml
# 중앙 집중식 로깅 설정

# Fluentd DaemonSet
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  namespace: kube-system
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule
      containers:
      - name: fluentd-elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v3.4.0
        env:
        - name: FLUENTD_SYSTEMD_CONF
          value: disable
        - name: FLUENTD_PROMETHEUS_CONF
          value: disable
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: config-volume
          mountPath: /etc/fluent/config.d
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: config-volume
        configMap:
          name: fluentd-config
```

## 🔒 보안 강화

### Pod Security Standards

```yaml
# pod-security-standards.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    # 엄격한 보안 정책
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
---
apiVersion: v1
kind: Namespace
metadata:
  name: monitoring
  labels:
    # 모니터링은 일부 권한 필요
    pod-security.kubernetes.io/enforce: baseline
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
```

### 네트워크 정책

```yaml
# network-policies.yaml
---
# 기본 거부 정책
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress

---
# 웹 애플리케이션 정책
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: web-app-policy
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: web-app
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 8080
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: database
    ports:
    - protocol: TCP
      port: 3306
  # DNS 허용
  - to: []
    ports:
    - protocol: UDP
      port: 53
```

## 📈 성능 최적화

### kubelet 튜닝

```yaml
# kubelet-config.yaml
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
# 이미지 가비지 컬렉션
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
# 컨테이너 가비지 컬렉션
containerGCThreshold:
  minAge: 0s
  maxPerPodContainer: 1
  maxContainers: -1
# 이벤트 관리
eventRecordQPS: 50
eventBurst: 100
# 리소스 예약
systemReserved:
  cpu: "0.5"
  memory: "1Gi"
  ephemeral-storage: "10Gi"
kubeReserved:
  cpu: "0.5"
  memory: "1Gi"
  ephemeral-storage: "10Gi"
# 메모리 관리
memorySwap: {}
memoryThrottlingFactor: 0.8
# 병렬 처리
maxPods: 250
podPidsLimit: 4096
# 로그 관리
containerLogMaxSize: 100Mi
containerLogMaxFiles: 5
```

### API 서버 튜닝

```yaml
# api-server-optimization.yaml
# /etc/kubernetes/manifests/kube-apiserver.yaml 수정 내용

spec:
  containers:
  - command:
    - kube-apiserver
    # 감사 로그 최적화
    - --audit-log-maxage=30
    - --audit-log-maxbackup=3
    - --audit-log-maxsize=100
    # 요청 제한
    - --max-requests-inflight=400
    - --max-mutating-requests-inflight=200
    # 워치 캐시 최적화
    - --watch-cache-sizes=nodes#100,pods#1000,services#100
    # etcd 연결 최적화
    - --etcd-compaction-interval=300s
    # API 우선순위 및 공정성
    - --enable-priority-and-fairness=true
    - --request-timeout=60s
```

## 🚨 장애 대응 절차

### 장애 대응 체크리스트

```markdown
# 장애 대응 체크리스트

## 1단계: 초기 대응 (5분 이내)
- [ ] 장애 범위 확인
- [ ] 클러스터 API 응답 확인
- [ ] 주요 서비스 상태 확인
- [ ] 모니터링 알림 확인

## 2단계: 상세 진단 (15분 이내)
- [ ] 노드 상태 확인
- [ ] 시스템 Pod 상태 확인
- [ ] 리소스 사용량 확인
- [ ] 이벤트 로그 분석

## 3단계: 복구 작업
- [ ] 영향받은 컴포넌트 재시작
- [ ] 트래픽 우회 (필요시)
- [ ] 백업에서 복원 (필요시)
- [ ] 서비스 검증

## 4단계: 사후 조치
- [ ] 근본 원인 분석
- [ ] 재발 방지 대책 수립
- [ ] 문서 업데이트
- [ ] 팀 내 공유
```

### 자동 복구 스크립트

```bash
#!/bin/bash
# auto-recovery.sh

# 자동 복구 스크립트 (신중하게 사용)

recover_node() {
    local node=$1
    
    echo "Attempting to recover node: $node"
    
    # 노드 드레인
    kubectl drain $node --ignore-daemonsets --delete-emptydir-data --force
    
    # SSH로 노드에서 kubelet 재시작
    ssh $node "systemctl restart kubelet containerd"
    
    # 노드 언코든
    kubectl uncordon $node
    
    echo "Node recovery attempted for: $node"
}

recover_system_pods() {
    echo "Checking system pods..."
    
    # 실패한 시스템 Pod 재시작
    kubectl get pods -n kube-system --field-selector=status.phase=Failed -o name | while read pod; do
        echo "Restarting failed system pod: $pod"
        kubectl delete $pod -n kube-system
    done
}

# 사용 예: ./auto-recovery.sh node worker-01
if [ "$1" = "node" ] && [ -n "$2" ]; then
    recover_node $2
elif [ "$1" = "pods" ]; then
    recover_system_pods
else
    echo "Usage: $0 {node <node-name>|pods}"
fi
```

---

> 💡 **실전 경험**: 온프렘 운영의 핵심은 예측 가능한 문제에 대한 철저한 준비입니다. 자동화는 점진적으로 도입하고, 항상 수동 복구 절차를 병행하세요. 백업과 모니터링은 절대 소홀히 하지 마세요.

태그: #bestpractices #operations #automation #onprem #production