# Prometheus ìŠ¤íƒ êµ¬ì¶•

## ğŸ¯ ê°œìš”

ì˜¨í”„ë ˜ ì¿ ë²„ë„¤í‹°ìŠ¤ í™˜ê²½ì—ì„œ Prometheus, Grafana, AlertManagerë¥¼ í™œìš©í•œ ì¢…í•© ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ì‹¤ì „ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ—ï¸ ëª¨ë‹ˆí„°ë§ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Grafana                           â”‚
â”‚            (Visualization & Dashboards)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Prometheus                           â”‚
â”‚             (Metrics Storage & Query)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚             â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚Node   â”‚    â”‚kube-   â”‚    â”‚Custom  â”‚
â”‚Exporterâ”‚   â”‚state-  â”‚    â”‚App     â”‚
â”‚       â”‚    â”‚metrics â”‚    â”‚Metrics â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Helmì„ ì´ìš©í•œ ì„¤ì¹˜

### Helm ì„¤ì¹˜

```bash
# Helm ì„¤ì¹˜
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# ë²„ì „ í™•ì¸
helm version

# Prometheus ì»¤ë®¤ë‹ˆí‹° ì°¨íŠ¸ ì €ì¥ì†Œ ì¶”ê°€
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update
```

### kube-prometheus-stack ì„¤ì¹˜

```yaml
# prometheus-values.yaml
# Prometheus, Grafana, AlertManager í†µí•© ì„¤ì¹˜

prometheus:
  prometheusSpec:
    # ì˜¨í”„ë ˜ í™˜ê²½ì—ì„œì˜ ë°ì´í„° ë³´ì¡´ ê¸°ê°„
    retention: 30d
    retentionSize: 50GB
    
    # ìŠ¤í† ë¦¬ì§€ ì„¤ì •
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: "local-storage"  # ì‹¤ì œ SCëª…ìœ¼ë¡œ ë³€ê²½
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi
    
    # ë¦¬ì†ŒìŠ¤ ì„¤ì •
    resources:
      requests:
        memory: 2Gi
        cpu: 500m
      limits:
        memory: 4Gi
        cpu: 1000m
    
    # ì™¸ë¶€ ë¼ë²¨ ì¶”ê°€
    externalLabels:
      cluster: "onprem-k8s"
      datacenter: "dc1"
    
    # ìŠ¤í¬ë˜í•‘ ê°„ê²©
    scrapeInterval: 30s
    evaluationInterval: 30s
    
    # ì¶”ê°€ ìŠ¤í¬ë˜í•‘ ì„¤ì •
    additionalScrapeConfigs:
      - job_name: 'kubernetes-nodes-custom'
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - source_labels: [__address__]
            regex: '(.*):10250'
            target_label: __address__
            replacement: '${1}:9100'

grafana:
  # ê´€ë¦¬ì ë¹„ë°€ë²ˆí˜¸ ì„¤ì •
  adminPassword: "admin123!@#"  # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Secret ì‚¬ìš©
  
  # ì˜êµ¬ ìŠ¤í† ë¦¬ì§€ ì„¤ì •
  persistence:
    enabled: true
    storageClassName: "local-storage"
    size: 10Gi
  
  # ì„œë¹„ìŠ¤ íƒ€ì… ì„¤ì • (ì˜¨í”„ë ˜)
  service:
    type: NodePort
    nodePort: 30300
  
  # Grafana ì„¤ì •
  grafana.ini:
    server:
      root_url: "http://grafana.company.local"  # ì‹¤ì œ ë„ë©”ì¸ìœ¼ë¡œ ë³€ê²½
    security:
      allow_embedding: true
    users:
      allow_sign_up: false
    auth:
      disable_login_form: false
  
  # ëŒ€ì‹œë³´ë“œ ìë™ import
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'default'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/default
  
  # ê¸°ë³¸ ëŒ€ì‹œë³´ë“œ
  dashboards:
    default:
      kubernetes-cluster:
        gnetId: 7249
        revision: 1
        datasource: Prometheus
      node-exporter:
        gnetId: 1860
        revision: 27
        datasource: Prometheus
      kubernetes-pods:
        gnetId: 6417
        revision: 1
        datasource: Prometheus

alertmanager:
  alertmanagerSpec:
    # ìŠ¤í† ë¦¬ì§€ ì„¤ì •
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: "local-storage"
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi
    
    # ì™¸ë¶€ ì ‘ê·¼ ì„¤ì •
    externalUrl: "http://alertmanager.company.local"
    
    # ë¦¬ì†ŒìŠ¤ ì œí•œ
    resources:
      requests:
        memory: 256Mi
        cpu: 100m
      limits:
        memory: 512Mi
        cpu: 200m

# Node Exporter ì„¤ì •
nodeExporter:
  enabled: true
  
# kube-state-metrics ì„¤ì •  
kubeStateMetrics:
  enabled: true

# CoreDNS ëª¨ë‹ˆí„°ë§ í™œì„±í™”
coreDns:
  enabled: true

# etcd ëª¨ë‹ˆí„°ë§ í™œì„±í™” (kubeadm í™˜ê²½)
kubeEtcd:
  enabled: true
  endpoints:
    - 192.168.1.10  # ë§ˆìŠ¤í„° ë…¸ë“œ IP
    - 192.168.1.11
    - 192.168.1.12
  service:
    enabled: true
    port: 2379
    targetPort: 2379
```

### ì„¤ì¹˜ ì‹¤í–‰

```bash
# ëª¨ë‹ˆí„°ë§ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
kubectl create namespace monitoring

# Prometheus ìŠ¤íƒ ì„¤ì¹˜
helm install prometheus-stack prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --values prometheus-values.yaml

# ì„¤ì¹˜ ìƒíƒœ í™•ì¸
kubectl get pods -n monitoring
kubectl get svc -n monitoring
```

## ğŸ”§ ì¶”ê°€ Exporter ì„¤ì¹˜

### Node Exporter (ë² ì–´ë©”íƒˆ ì„œë²„ìš©)

```bash
# ë² ì–´ë©”íƒˆ ì„œë²„ì— Node Exporter ì„¤ì¹˜
# ê° ë¬¼ë¦¬ ì„œë²„ì—ì„œ ì‹¤í–‰

# ì‚¬ìš©ì ìƒì„±
useradd --no-create-home --shell /bin/false node_exporter

# ë°”ì´ë„ˆë¦¬ ë‹¤ìš´ë¡œë“œ
wget https://github.com/prometheus/node_exporter/releases/download/v1.6.1/node_exporter-1.6.1.linux-amd64.tar.gz
tar xvfz node_exporter-1.6.1.linux-amd64.tar.gz
sudo mv node_exporter-1.6.1.linux-amd64/node_exporter /usr/local/bin/
chown node_exporter:node_exporter /usr/local/bin/node_exporter

# ì„œë¹„ìŠ¤ íŒŒì¼ ìƒì„±
cat <<EOF | sudo tee /etc/systemd/system/node_exporter.service
[Unit]
Description=Node Exporter
Wants=network-online.target
After=network-online.target

[Service]
User=node_exporter
Group=node_exporter
Type=simple
ExecStart=/usr/local/bin/node_exporter \
  --collector.systemd \
  --collector.processes \
  --collector.interrupts

[Install]
WantedBy=multi-user.target
EOF

# ì„œë¹„ìŠ¤ ì‹œì‘
systemctl daemon-reload
systemctl enable node_exporter
systemctl start node_exporter

# ìƒíƒœ í™•ì¸
systemctl status node_exporter
curl http://localhost:9100/metrics
```

### í•˜ë“œì›¨ì–´ ëª¨ë‹ˆí„°ë§ (IPMI)

```yaml
# ipmi-exporter.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: ipmi-exporter
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: ipmi-exporter
  template:
    metadata:
      labels:
        app: ipmi-exporter
    spec:
      hostNetwork: true
      hostPID: true
      containers:
      - name: ipmi-exporter
        image: prometheuscommunity/ipmi-exporter:v1.6.1
        ports:
        - containerPort: 9290
          hostPort: 9290
        args:
        - "--config.file=/etc/ipmi_exporter/config.yaml"
        securityContext:
          privileged: true
        volumeMounts:
        - name: dev
          mountPath: /dev
        - name: config
          mountPath: /etc/ipmi_exporter
      volumes:
      - name: dev
        hostPath:
          path: /dev
      - name: config
        configMap:
          name: ipmi-exporter-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ipmi-exporter-config
  namespace: monitoring
data:
  config.yaml: |
    modules:
      default:
        collectors:
        - ipmi
        - dcmi
        - bmc
        exclude_sensor_ids:
        - 2
```

## ğŸ“Š ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ì„¤ì •

### ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”íŠ¸ë¦­

```yaml
# custom-app-monitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: custom-app-monitor
  namespace: monitoring
  labels:
    app: custom-app
    release: prometheus-stack
spec:
  selector:
    matchLabels:
      app: custom-app
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
    scheme: http
---
apiVersion: v1
kind: Service
metadata:
  name: custom-app-metrics
  namespace: default
  labels:
    app: custom-app
spec:
  selector:
    app: custom-app
  ports:
  - name: metrics
    port: 8080
    targetPort: 8080
```

### JMX ë©”íŠ¸ë¦­ (Java ì• í”Œë¦¬ì¼€ì´ì…˜)

```yaml
# jmx-exporter.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: jmx-exporter-config
  namespace: monitoring
data:
  config.yaml: |
    rules:
    - pattern: ".*"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jmx-exporter
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jmx-exporter
  template:
    metadata:
      labels:
        app: jmx-exporter
    spec:
      containers:
      - name: jmx-exporter
        image: sscaling/jmx-prometheus-exporter:0.17.2
        ports:
        - containerPort: 5556
        env:
        - name: CONFIG_YML
          value: "/etc/jmx-exporter/config.yaml"
        - name: JVM_OPTS
          value: "-Xmx512m"
        volumeMounts:
        - name: config
          mountPath: /etc/jmx-exporter
      volumes:
      - name: config
        configMap:
          name: jmx-exporter-config
```

## ğŸš¨ AlertManager ì„¤ì •

### ì•Œë¦¼ ê·œì¹™ ì •ì˜

```yaml
# custom-alerts.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: custom-alerts
  namespace: monitoring
  labels:
    prometheus: kube-prometheus
    role: alert-rules
spec:
  groups:
  - name: kubernetes-cluster
    rules:
    - alert: NodeDown
      expr: up{job="node-exporter"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Node {{ $labels.instance }} is down"
        description: "Node {{ $labels.instance }} has been down for more than 5 minutes."
        
    - alert: HighCPUUsage
      expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "High CPU usage on {{ $labels.instance }}"
        description: "CPU usage is above 80% for more than 10 minutes."
        
    - alert: HighMemoryUsage
      expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High memory usage on {{ $labels.instance }}"
        description: "Memory usage is above 90% for more than 5 minutes."
        
    - alert: DiskSpaceLow
      expr: (1 - (node_filesystem_avail_bytes{fstype!="tmpfs"} / node_filesystem_size_bytes{fstype!="tmpfs"})) * 100 > 85
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Low disk space on {{ $labels.instance }}"
        description: "Disk usage is above 85% on filesystem {{ $labels.mountpoint }}."
        
    - alert: PodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
        description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been restarting frequently."
        
  - name: etcd
    rules:
    - alert: etcdBackendCommitDurationHigh
      expr: histogram_quantile(0.99, sum(rate(etcd_disk_backend_commit_duration_seconds_bucket[5m])) by (instance, le)) > 0.25
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "etcd backend commit duration is high"
        description: "99th percentile of etcd disk backend commit duration is {{ $value }}s on {{ $labels.instance }}."
```

### AlertManager ì„¤ì •

```yaml
# alertmanager-config.yaml
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-prometheus-stack-kube-prom-alertmanager
  namespace: monitoring
type: Opaque
stringData:
  alertmanager.yml: |
    global:
      smtp_smarthost: 'mail.company.local:587'
      smtp_from: 'alerts@company.local'
      smtp_auth_username: 'alerts@company.local'
      smtp_auth_password: 'password123'
      
    templates:
    - '/etc/alertmanager/templates/*.tmpl'
    
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10m
      repeat_interval: 12h
      receiver: 'web.hook'
      routes:
      - match:
          severity: critical
        receiver: critical-alerts
        continue: true
      - match:
          severity: warning
        receiver: warning-alerts
        
    receivers:
    - name: 'web.hook'
      webhook_configs:
      - url: 'http://webhook.company.local/alerts'
        
    - name: 'critical-alerts'
      email_configs:
      - to: 'oncall@company.local'
        subject: '[CRITICAL] {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Labels: {{ range .Labels.SortedPairs }}{{ .Name }}={{ .Value }} {{ end }}
          {{ end }}
      slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        channel: '#alerts-critical'
        title: 'Critical Alert'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        
    - name: 'warning-alerts'
      email_configs:
      - to: 'team@company.local'
        subject: '[WARNING] {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          {{ end }}
```

## ğŸ“ˆ Grafana ëŒ€ì‹œë³´ë“œ ì»¤ìŠ¤í„°ë§ˆì´ì§•

### ì˜¨í”„ë ˜ í´ëŸ¬ìŠ¤í„° ëŒ€ì‹œë³´ë“œ

```json
{
  "dashboard": {
    "id": null,
    "title": "OnPrem Kubernetes Cluster Overview",
    "tags": ["kubernetes", "onprem"],
    "timezone": "Asia/Seoul",
    "panels": [
      {
        "id": 1,
        "title": "Cluster Nodes Status",
        "type": "stat",
        "targets": [
          {
            "expr": "kube_node_status_condition{condition=\"Ready\",status=\"true\"}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "thresholds"
            },
            "thresholds": {
              "steps": [
                {"color": "red", "value": 0},
                {"color": "green", "value": 1}
              ]
            }
          }
        }
      },
      {
        "id": 2,
        "title": "CPU Usage by Node",
        "type": "timeseries",
        "targets": [
          {
            "expr": "100 - (avg by(instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)"
          }
        ]
      },
      {
        "id": 3,
        "title": "Memory Usage by Node",
        "type": "timeseries", 
        "targets": [
          {
            "expr": "(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100"
          }
        ]
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "30s"
  }
}
```

## ğŸ” ëª¨ë‹ˆí„°ë§ ê²€ì¦

### ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í™•ì¸

```bash
# Prometheus íƒ€ê²Ÿ ìƒíƒœ í™•ì¸
kubectl port-forward -n monitoring svc/prometheus-stack-kube-prom-prometheus 9090:9090

# ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:9090/targets ì ‘ì†í•˜ì—¬ í™•ì¸

# ì£¼ìš” ë©”íŠ¸ë¦­ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
curl -G 'http://localhost:9090/api/v1/query' \
  --data-urlencode 'query=up{job="node-exporter"}'

curl -G 'http://localhost:9090/api/v1/query' \
  --data-urlencode 'query=kube_node_status_condition{condition="Ready"}'
```

### Grafana ì ‘ì† ë° ì„¤ì •

```bash
# Grafana ì ‘ì†
kubectl port-forward -n monitoring svc/prometheus-stack-grafana 3000:80

# ê¸°ë³¸ ë¡œê·¸ì¸: admin / admin123!@#
# http://localhost:3000 ì ‘ì†

# Prometheus ë°ì´í„°ì†ŒìŠ¤ í™•ì¸
# Configuration > Data Sources > Prometheus
# URL: http://prometheus-stack-kube-prom-prometheus:9090
```

## ğŸ› ï¸ ì„±ëŠ¥ íŠœë‹

### Prometheus ìµœì í™”

```yaml
# prometheus-performance.yaml
prometheus:
  prometheusSpec:
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
    resources:
      requests:
        memory: 4Gi
        cpu: 1000m
      limits:
        memory: 8Gi
        cpu: 2000m
    
    # ì••ì¶• ë° ë³´ì¡´ ì •ì±…
    retention: 15d
    retentionSize: 40GB
    
    # WAL ì••ì¶•
    walCompression: true
    
    # ìŠ¤í¬ë˜í•‘ ìµœì í™”
    scrapeInterval: 30s
    scrapeTimeout: 10s
    
    # ì¿¼ë¦¬ íƒ€ì„ì•„ì›ƒ
    queryTimeout: 30s
    
    # ë™ì‹œ ì¿¼ë¦¬ ì œí•œ
    queryConcurrency: 20
    
    # ë©”ëª¨ë¦¬ ì œí•œ
    queryMaxConcurrency: 20
    queryMaxSamples: 50000000
```

### ì¥ê¸° ìŠ¤í† ë¦¬ì§€ ì„¤ì • (Thanos)

```yaml
# thanos-sidecar.yaml
prometheus:
  prometheusSpec:
    thanos:
      image: quay.io/thanos/thanos:v0.32.2
      version: v0.32.2
      objectStorageConfig:
        key: thanos.yaml
        name: thanos-objstore-secret
```

## ğŸš¨ ì‹¤ì „ ìš´ì˜ íŒ

### ğŸ”§ ë©”íŠ¸ë¦­ ìµœì í™”
1. **ë¶ˆí•„ìš”í•œ ë©”íŠ¸ë¦­ ì œê±°**: `metric_relabel_configs` í™œìš©
2. **ìƒ˜í”Œë§ ê°„ê²© ì¡°ì •**: ì¤‘ìš”ë„ì— ë”°ë¼ ê°„ê²© ì°¨ë³„í™”
3. **ë©”ëª¨ë¦¬ ê´€ë¦¬**: Prometheus ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

### ğŸ“Š ëŒ€ì‹œë³´ë“œ ê´€ë¦¬
1. **í‘œì¤€í™”**: íŒ€ ë‚´ ëŒ€ì‹œë³´ë“œ í…œí”Œë¦¿ ì •ì˜
2. **ì„±ëŠ¥ ê³ ë ¤**: ë¬´ê±°ìš´ ì¿¼ë¦¬ ìµœì†Œí™”
3. **ì‚¬ìš©ìë³„ ì ‘ê·¼**: RBAC ì ìš©

### ğŸš¨ ì•Œë¦¼ ì •ì±…
1. **ì•Œë¦¼ í”¼ë¡œ ë°©ì§€**: ì ì ˆí•œ ì„ê³„ê°’ê³¼ ì§€ì—° ì‹œê°„ ì„¤ì •
2. **ì—ìŠ¤ì»¬ë ˆì´ì…˜**: ì‹¬ê°ë„ë³„ ì•Œë¦¼ ëŒ€ìƒ ë¶„ë¦¬
3. **ì¹¨ë¬µ ê·œì¹™**: ì ê²€ ì‹œê°„ ì•Œë¦¼ ì°¨ë‹¨

---

> ğŸ’¡ **ì‹¤ì „ ê²½í—˜**: ì˜¨í”„ë ˜ í™˜ê²½ì—ì„œëŠ” ìŠ¤í† ë¦¬ì§€ ìš©ëŸ‰ ê´€ë¦¬ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤. ë©”íŠ¸ë¦­ ë³´ì¡´ ê¸°ê°„ê³¼ í•´ìƒë„ë¥¼ ì‹ ì¤‘íˆ ì„¤ì •í•˜ê³ , ì •ê¸°ì ì¸ ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ì„ ìˆ˜í–‰í•˜ì„¸ìš”.

íƒœê·¸: #monitoring #prometheus #grafana #alertmanager #onprem