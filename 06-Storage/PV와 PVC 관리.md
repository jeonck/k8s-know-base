# PVì™€ PVC ê´€ë¦¬

## ğŸ¯ ê°œìš”

ì¿ ë²„ë„¤í‹°ìŠ¤ ìŠ¤í† ë¦¬ì§€ì˜ í•µì‹¬ì¸ PersistentVolume(PV)ê³¼ PersistentVolumeClaim(PVC)ì˜ ìƒëª…ì£¼ê¸° ê´€ë¦¬, ìµœì í™” ì „ëµ, ê·¸ë¦¬ê³  ì˜¨í”„ë ˜ í™˜ê²½ì—ì„œì˜ ì‹¤ì „ ìš´ì˜ ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.

## ğŸ“š PVì™€ PVC ê¸°ë³¸ ê°œë…

### ìŠ¤í† ë¦¬ì§€ ë ˆì´ì–´ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Application   â”‚ â† Podì—ì„œ ë§ˆìš´íŠ¸ ê²½ë¡œ ì‚¬ìš©
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      PVC        â”‚ â† ìŠ¤í† ë¦¬ì§€ ìš”ì²­ (í´ë ˆì„)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚       PV        â”‚ â† ì‹¤ì œ ìŠ¤í† ë¦¬ì§€ ë¦¬ì†ŒìŠ¤
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Storage Backend â”‚ â† ë¬¼ë¦¬ì  ìŠ¤í† ë¦¬ì§€ (NFS, Ceph, etc.)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### PV ìƒëª…ì£¼ê¸°

```
Available â†’ Bound â†’ Released â†’ (Recycled/Deleted/Failed)
```

## ğŸ”§ PV ì„¤ì •ê³¼ ê´€ë¦¬

### Static PV ìƒì„±

```yaml
# nfs-pv-example.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv-web-data
  labels:
    type: nfs
    environment: production
    tier: frontend
spec:
  capacity:
    storage: 100Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: nfs-storage
  mountOptions:
    - hard
    - nfsvers=4.1
    - rsize=1048576
    - wsize=1048576
    - timeo=600
    - retrans=2
  nfs:
    path: /export/web-data
    server: 192.168.1.100

---
# ë¡œì»¬ ìŠ¤í† ë¦¬ì§€ PV ì˜ˆì œ
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-ssd-01
  labels:
    type: local
    performance: high
spec:
  capacity:
    storage: 500Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-ssd
  local:
    path: /mnt/ssd-storage
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - worker-node-01

---
# Ceph RBD PV ì˜ˆì œ
apiVersion: v1
kind: PersistentVolume
metadata:
  name: ceph-rbd-pv-01
spec:
  capacity:
    storage: 200Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: ceph-rbd
  rbd:
    monitors:
      - 192.168.1.10:6789
      - 192.168.1.11:6789
      - 192.168.1.12:6789
    pool: rbd
    image: kube-image-01
    user: admin
    secretRef:
      name: ceph-secret
    fsType: ext4
    readOnly: false
```

### PV ê´€ë¦¬ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# pv-management.sh

# PV ìƒíƒœ ëª¨ë‹ˆí„°ë§
monitor_pv_status() {
    echo "=== PV Status Monitor ==="
    echo "Timestamp: $(date)"
    echo
    
    # ì „ì²´ PV ìƒíƒœ
    echo "=== All PVs ==="
    kubectl get pv -o wide
    echo
    
    # ìƒíƒœë³„ PV ì¹´ìš´íŠ¸
    echo "=== PV Status Summary ==="
    echo "Available: $(kubectl get pv --no-headers | grep Available | wc -l)"
    echo "Bound: $(kubectl get pv --no-headers | grep Bound | wc -l)"
    echo "Released: $(kubectl get pv --no-headers | grep Released | wc -l)"
    echo "Failed: $(kubectl get pv --no-headers | grep Failed | wc -l)"
    echo
    
    # ìš©ëŸ‰ë³„ í†µê³„
    echo "=== Storage Capacity Summary ==="
    kubectl get pv --no-headers -o custom-columns=CAPACITY:.spec.capacity.storage | sort | uniq -c
    echo
    
    # ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ë³„ ë¶„í¬
    echo "=== PVs by Storage Class ==="
    kubectl get pv --no-headers -o custom-columns=SC:.spec.storageClassName | sort | uniq -c
}

# Released ìƒíƒœ PV ì •ë¦¬
cleanup_released_pvs() {
    echo "=== Cleaning up Released PVs ==="
    
    released_pvs=$(kubectl get pv --no-headers | grep Released | awk '{print $1}')
    
    if [ -z "$released_pvs" ]; then
        echo "No Released PVs found"
        return
    fi
    
    echo "Found Released PVs:"
    echo "$released_pvs"
    echo
    
    read -p "Do you want to delete these PVs? (y/N): " -n 1 -r
    echo
    
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        for pv in $released_pvs; do
            echo "Deleting PV: $pv"
            kubectl delete pv "$pv"
        done
    else
        echo "Cleanup cancelled"
    fi
}

# PV ë°±ì—… ì •ë³´ ìƒì„±
backup_pv_info() {
    local backup_dir="/var/backups/k8s-pv"
    local timestamp=$(date +%Y%m%d_%H%M%S)
    
    mkdir -p "$backup_dir"
    
    echo "Backing up PV information..."
    kubectl get pv -o yaml > "$backup_dir/pv-backup-$timestamp.yaml"
    kubectl get pvc --all-namespaces -o yaml > "$backup_dir/pvc-backup-$timestamp.yaml"
    
    echo "Backup completed: $backup_dir/pv-backup-$timestamp.yaml"
}

case "${1:-status}" in
    status)
        monitor_pv_status
        ;;
    cleanup)
        cleanup_released_pvs
        ;;
    backup)
        backup_pv_info
        ;;
    *)
        echo "Usage: $0 {status|cleanup|backup}"
        exit 1
        ;;
esac
```

## ğŸ“‹ PVC ì„¤ì •ê³¼ ê´€ë¦¬

### PVC ìƒì„± ì˜ˆì œ

```yaml
# basic-pvc-examples.yaml
---
# ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ìš© PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: web-app-storage
  namespace: production
  labels:
    app: web-app
    tier: frontend
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 50Gi
  storageClassName: nfs-storage
  selector:
    matchLabels:
      type: nfs
      tier: frontend

---
# ë°ì´í„°ë² ì´ìŠ¤ìš© ê³ ì„±ëŠ¥ PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-data
  namespace: database
  labels:
    app: mysql
    tier: database
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 200Gi
  storageClassName: local-ssd
  selector:
    matchLabels:
      type: local
      performance: high

---
# ë¡œê·¸ ìˆ˜ì§‘ìš© PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: log-storage
  namespace: monitoring
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 500Gi
  storageClassName: nfs-storage
```

### PVC í™•ì¥ (Volume Expansion)

```yaml
# ê¸°ì¡´ PVC í™•ì¥
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: expandable-storage
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi  # ê¸°ì¡´ 50Giì—ì„œ 100Gië¡œ í™•ì¥
  storageClassName: expandable-storage-class
```

```bash
# PVC í™•ì¥ ìŠ¤í¬ë¦½íŠ¸
#!/bin/bash
# expand-pvc.sh

PVC_NAME=$1
NAMESPACE=$2
NEW_SIZE=$3

if [ $# -ne 3 ]; then
    echo "Usage: $0 <pvc-name> <namespace> <new-size>"
    echo "Example: $0 mysql-data default 100Gi"
    exit 1
fi

echo "Expanding PVC $PVC_NAME in namespace $NAMESPACE to $NEW_SIZE"

# í˜„ì¬ í¬ê¸° í™•ì¸
current_size=$(kubectl get pvc $PVC_NAME -n $NAMESPACE -o jsonpath='{.spec.resources.requests.storage}')
echo "Current size: $current_size"

# ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ í™•ì¥ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
storage_class=$(kubectl get pvc $PVC_NAME -n $NAMESPACE -o jsonpath='{.spec.storageClassName}')
allow_expansion=$(kubectl get storageclass $storage_class -o jsonpath='{.allowVolumeExpansion}')

if [ "$allow_expansion" != "true" ]; then
    echo "Error: Storage class $storage_class does not allow volume expansion"
    exit 1
fi

# PVC í™•ì¥ ì‹¤í–‰
kubectl patch pvc $PVC_NAME -n $NAMESPACE -p "{\"spec\":{\"resources\":{\"requests\":{\"storage\":\"$NEW_SIZE\"}}}}"

echo "PVC expansion initiated. Monitoring progress..."

# í™•ì¥ ìƒíƒœ ëª¨ë‹ˆí„°ë§
timeout=300
while [ $timeout -gt 0 ]; do
    conditions=$(kubectl get pvc $PVC_NAME -n $NAMESPACE -o jsonpath='{.status.conditions[*].type}')
    if echo "$conditions" | grep -q "FileSystemResizePending"; then
        echo "File system resize pending... waiting for Pod restart"
    elif echo "$conditions" | grep -q "Resizing"; then
        echo "Volume resize in progress..."
    else
        new_size=$(kubectl get pvc $PVC_NAME -n $NAMESPACE -o jsonpath='{.status.capacity.storage}')
        echo "Expansion completed. New size: $new_size"
        break
    fi
    
    sleep 10
    timeout=$((timeout - 10))
done

if [ $timeout -le 0 ]; then
    echo "Warning: Expansion timeout. Check PVC status manually."
fi
```

## ğŸ”„ Dynamic Provisioning

### StorageClass ì™€ Dynamic PV

```yaml
# dynamic-storage-classes.yaml
---
# NFS Dynamic Provisioner
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-dynamic
  annotations:
    storageclass.kubernetes.io/is-default-class: "false"
provisioner: example.com/nfs
parameters:
  server: 192.168.1.100
  path: /export/dynamic
  archiveOnDelete: "false"
allowVolumeExpansion: true
volumeBindingMode: Immediate
reclaimPolicy: Delete

---
# Local SSD Dynamic Provisioner  
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-ssd-dynamic
provisioner: kubernetes.io/no-provisioner
parameters:
  type: "ssd"
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: false
reclaimPolicy: Delete

---
# Ceph RBD Dynamic Provisioner
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ceph-rbd-dynamic
provisioner: rbd.csi.ceph.com
parameters:
  clusterID: ceph-cluster-id
  pool: rbd
  imageFormat: "2"
  imageFeatures: layering
  csi.storage.k8s.io/provisioner-secret-name: ceph-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: ceph-csi-rbd
  csi.storage.k8s.io/controller-expand-secret-name: ceph-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: ceph-csi-rbd
  csi.storage.k8s.io/node-stage-secret-name: ceph-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: ceph-csi-rbd
allowVolumeExpansion: true
volumeBindingMode: Immediate
reclaimPolicy: Delete
```

### NFS Provisioner ë°°í¬

```yaml
# nfs-provisioner.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nfs-client-provisioner
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: example.com/nfs
            - name: NFS_SERVER
              value: 192.168.1.100
            - name: NFS_PATH
              value: /export/dynamic
      volumes:
        - name: nfs-client-root
          nfs:
            server: 192.168.1.100
            path: /export/dynamic

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
  namespace: kube-system

---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner
rules:
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io

---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  namespace: kube-system
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]

---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  namespace: kube-system
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    namespace: kube-system
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io
```

## ğŸ“Š ìŠ¤í† ë¦¬ì§€ ëª¨ë‹ˆí„°ë§ê³¼ ê´€ë¦¬

### PVC ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

```bash
#!/bin/bash
# storage-monitoring.sh

echo "=== Storage Usage Monitoring ==="
echo "Timestamp: $(date)"
echo

# PVC ì‚¬ìš©ëŸ‰ ìƒì„¸ ì •ë³´
echo "=== PVC Usage Details ==="
kubectl get pvc --all-namespaces -o custom-columns="NAMESPACE:.metadata.namespace,NAME:.metadata.name,STATUS:.status.phase,VOLUME:.spec.volumeName,CAPACITY:.status.capacity.storage,STORAGECLASS:.spec.storageClassName"
echo

# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ ìŠ¤í† ë¦¬ì§€ ì‚¬ìš©ëŸ‰ ìš”ì•½
echo "=== Storage Usage by Namespace ==="
for ns in $(kubectl get namespaces -o name | cut -d/ -f2); do
    pvc_count=$(kubectl get pvc -n $ns --no-headers 2>/dev/null | wc -l)
    if [ $pvc_count -gt 0 ]; then
        echo "Namespace: $ns"
        kubectl get pvc -n $ns --no-headers -o custom-columns="NAME:.metadata.name,CAPACITY:.status.capacity.storage"
        echo
    fi
done

# ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ë³„ ì‚¬ìš©ëŸ‰
echo "=== Usage by Storage Class ==="
kubectl get pvc --all-namespaces --no-headers -o custom-columns="SC:.spec.storageClassName,CAPACITY:.status.capacity.storage" | sort | awk '
{
    sc = $1
    capacity = $2
    if (sc in total) {
        total[sc] = total[sc] " + " capacity
    } else {
        total[sc] = capacity
        count[sc] = 0
    }
    count[sc]++
}
END {
    for (sc in total) {
        printf "%-20s: %d PVCs\n", sc, count[sc]
    }
}'

# ë¬¸ì œê°€ ìˆëŠ” PVC í™•ì¸
echo "=== Problematic PVCs ==="
kubectl get pvc --all-namespaces --field-selector status.phase!=Bound 2>/dev/null || echo "All PVCs are Bound"
```

### ìŠ¤í† ë¦¬ì§€ ì •ë¦¬ ë° ìµœì í™”

```bash
#!/bin/bash
# storage-cleanup.sh

# ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” PV ì°¾ê¸°
find_unused_pvs() {
    echo "=== Finding Unused PVs ==="
    
    # Available ìƒíƒœì˜ PVë“¤
    available_pvs=$(kubectl get pv --no-headers | grep Available | awk '{print $1}')
    if [ -n "$available_pvs" ]; then
        echo "Available PVs (not bound to any PVC):"
        echo "$available_pvs"
        echo
    fi
    
    # Released ìƒíƒœì˜ PVë“¤  
    released_pvs=$(kubectl get pv --no-headers | grep Released | awk '{print $1}')
    if [ -n "$released_pvs" ]; then
        echo "Released PVs (previously bound, now released):"
        echo "$released_pvs"
        echo
    fi
}

# ì˜¤ë˜ëœ ì™„ë£Œëœ Podì˜ PVC í™•ì¸
find_stale_pvcs() {
    echo "=== Finding Potentially Stale PVCs ==="
    
    # ì™„ë£Œëœ Job/Podì™€ ì—°ê²°ëœ PVC ì°¾ê¸°
    for ns in $(kubectl get namespaces -o name | cut -d/ -f2); do
        completed_pods=$(kubectl get pods -n $ns --no-headers | grep -E 'Completed|Succeeded' | awk '{print $1}')
        
        for pod in $completed_pods; do
            pvcs=$(kubectl get pod $pod -n $ns -o jsonpath='{.spec.volumes[*].persistentVolumeClaim.claimName}' 2>/dev/null)
            if [ -n "$pvcs" ]; then
                echo "Namespace $ns, Completed Pod $pod uses PVCs: $pvcs"
            fi
        done
    done
}

# ì„ì‹œ ìŠ¤í† ë¦¬ì§€ ì •ë¦¬
cleanup_temp_storage() {
    echo "=== Cleaning up temporary storage ==="
    
    # ê° ë…¸ë“œì—ì„œ ì„ì‹œ ìŠ¤í† ë¦¬ì§€ ì •ë¦¬
    for node in $(kubectl get nodes -o name | cut -d/ -f2); do
        echo "Cleaning node: $node"
        
        # SSHë¡œ ì ‘ê·¼ ê°€ëŠ¥í•œ ê²½ìš° ì •ë¦¬ ì‹¤í–‰
        ssh $node "
            # containerd ì´ë¯¸ì§€ ì •ë¦¬
            crictl rmi --prune
            
            # ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë³¼ë¥¨ ì •ë¦¬
            find /var/lib/kubelet/pods -name 'volumes' -type d -empty -delete 2>/dev/null || true
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            find /tmp -name 'kubectl-*' -mtime +7 -delete 2>/dev/null || true
            
            echo 'Node $node cleanup completed'
        " 2>/dev/null || echo "SSH access not available for node $node"
    done
}

case "${1:-check}" in
    check)
        find_unused_pvs
        find_stale_pvcs
        ;;
    cleanup)
        cleanup_temp_storage
        ;;
    *)
        echo "Usage: $0 {check|cleanup}"
        exit 1
        ;;
esac
```

## ğŸ”’ PV/PVC ë³´ì•ˆ ê´€ë¦¬

### ì ‘ê·¼ ê¶Œí•œ ì œì–´

```yaml
# pvc-rbac.yaml
---
# PVC ìƒì„± ê¶Œí•œì„ ê°€ì§„ Role
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: production
  name: pvc-manager
rules:
- apiGroups: [""]
  resources: ["persistentvolumeclaims"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: [""]
  resources: ["persistentvolumes"]
  verbs: ["get", "list", "watch"]

---
# ì½ê¸° ì „ìš© PVC ì ‘ê·¼ ê¶Œí•œ
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: production
  name: pvc-viewer
rules:
- apiGroups: [""]
  resources: ["persistentvolumeclaims"]
  verbs: ["get", "list", "watch"]

---
# RoleBinding ì˜ˆì œ
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: pvc-managers
  namespace: production
subjects:
- kind: User
  name: storage-admin
  apiGroup: rbac.authorization.k8s.io
- kind: ServiceAccount
  name: storage-operator
  namespace: production
roleRef:
  kind: Role
  name: pvc-manager
  apiGroup: rbac.authorization.k8s.io
```

### ìŠ¤í† ë¦¬ì§€ ì •ì±… ì ìš©

```yaml
# storage-policies.yaml
---
# ResourceQuotaë¡œ ìŠ¤í† ë¦¬ì§€ ì‚¬ìš©ëŸ‰ ì œí•œ
apiVersion: v1
kind: ResourceQuota
metadata:
  name: storage-quota
  namespace: development
spec:
  hard:
    requests.storage: "500Gi"
    persistentvolumeclaims: "10"

---
# LimitRangeë¡œ PVC í¬ê¸° ì œí•œ
apiVersion: v1
kind: LimitRange
metadata:
  name: pvc-limit-range
  namespace: development
spec:
  limits:
  - type: PersistentVolumeClaim
    max:
      storage: "100Gi"
    min:
      storage: "1Gi"
    default:
      storage: "10Gi"
    defaultRequest:
      storage: "5Gi"
```

## ğŸ“‹ ìš´ì˜ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### 1. PV/PVC ëª…ëª… ê·œì¹™

```bash
# ê¶Œì¥ ëª…ëª… ê·œì¹™
# PV: <ìŠ¤í† ë¦¬ì§€íƒ€ì…>-<í™˜ê²½>-<ìš©ë„>-<ì¼ë ¨ë²ˆí˜¸>
nfs-prod-web-data-001
local-ssd-prod-db-mysql-001
ceph-rbd-staging-logs-001

# PVC: <ì• í”Œë¦¬ì¼€ì´ì…˜>-<ë°ì´í„°íƒ€ì…>-<í™˜ê²½>
wordpress-data-prod
mysql-data-staging
redis-cache-dev
```

### 2. ë ˆì´ë¸”ë§ ì „ëµ

```yaml
# í‘œì¤€ ë ˆì´ë¸” ì˜ˆì œ
metadata:
  labels:
    # ì• í”Œë¦¬ì¼€ì´ì…˜ ì •ë³´
    app.kubernetes.io/name: mysql
    app.kubernetes.io/instance: mysql-primary
    app.kubernetes.io/component: database
    
    # ìŠ¤í† ë¦¬ì§€ ê´€ë ¨
    storage.company.com/type: database
    storage.company.com/performance: high
    storage.company.com/backup-policy: daily
    
    # í™˜ê²½ ì •ë³´
    environment: production
    tier: data
```

### 3. ë°±ì—… ë° ë³µêµ¬ ì „ëµ

```bash
# PV ë°±ì—… ìŠ¤í¬ë¦½íŠ¸
#!/bin/bash
backup_pv_data() {
    local pv_name=$1
    local backup_path="/backups/pv-data"
    local timestamp=$(date +%Y%m%d_%H%M%S)
    
    # PV ì •ë³´ íšë“
    local mount_path=$(kubectl get pv $pv_name -o jsonpath='{.spec.hostPath.path}')
    local node=$(kubectl get pv $pv_name -o jsonpath='{.spec.nodeAffinity.required.nodeSelectorTerms[0].matchExpressions[0].values[0]}')
    
    # ë°ì´í„° ë°±ì—…
    ssh $node "tar czf $backup_path/$pv_name-$timestamp.tar.gz -C $mount_path ."
    
    echo "Backup completed: $backup_path/$pv_name-$timestamp.tar.gz"
}
```

### 4. ëª¨ë‹ˆí„°ë§ ì„¤ì •

```yaml
# PrometheusRule for PV/PVC monitoring
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: storage-monitoring
  namespace: monitoring
spec:
  groups:
  - name: storage.rules
    rules:
    - alert: PVCPendingForTooLong
      expr: kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
      for: 5m
      annotations:
        summary: "PVC {{ $labels.persistentvolumeclaim }} is pending for too long"
        
    - alert: PVDiskSpaceUsageHigh
      expr: (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) > 0.9
      for: 2m
      annotations:
        summary: "PV disk usage is above 90%"
```

---

> ğŸ’¡ **ì‹¤ì „ ê²½í—˜**: PV/PVC ê´€ë¦¬ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ consistentí•œ ëª…ëª… ê·œì¹™ê³¼ ë ˆì´ë¸”ë§ì…ë‹ˆë‹¤. ì´ˆê¸°ì— ì˜ ì„¤ê³„í•´ë‘ë©´ ë‚˜ì¤‘ì— ìë™í™” ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±ê³¼ ëª¨ë‹ˆí„°ë§ì´ í›¨ì”¬ ì‰¬ì›Œì§‘ë‹ˆë‹¤.

íƒœê·¸: #storage #pv #pvc #management #onprem #backup
