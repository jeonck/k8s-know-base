# 온프렘 스토리지 솔루션

## 🎯 개요

온프렘 쿠버네티스 환경에서 사용할 수 있는 다양한 스토리지 솔루션들의 특징, 구축 방법, 성능 최적화, 그리고 실전 운영 경험을 다룹니다.

## 📊 스토리지 솔루션 비교

### 주요 온프렘 스토리지 옵션

| 솔루션 | 타입 | 성능 | 확장성 | 복잡도 | 가용성 | 비용 |
|--------|------|------|--------|--------|--------|------|
| **NFS** | Network | 중간 | 제한적 | 낮음 | 중간 | 낮음 |
| **Ceph** | Distributed | 높음 | 우수 | 높음 | 높음 | 중간 |
| **GlusterFS** | Distributed | 중간 | 우수 | 중간 | 높음 | 낮음 |
| **Longhorn** | Cloud-Native | 중간 | 우수 | 낮음 | 높음 | 낮음 |
| **Local Storage** | Local | 매우높음 | 없음 | 낮음 | 낮음 | 낮음 |
| **iSCSI** | Network | 높음 | 중간 | 중간 | 중간 | 중간 |

## 🗄️ NFS 스토리지 구성

### NFS 서버 설정

```bash
#!/bin/bash
# setup-nfs-server.sh

# NFS 서버 설치 및 설정 (CentOS/RHEL)
setup_nfs_server() {
    echo "Setting up NFS server..."
    
    # NFS 패키지 설치
    yum install -y nfs-utils nfs-utils-lib
    
    # NFS 서비스 시작
    systemctl enable nfs-server
    systemctl start nfs-server
    systemctl enable rpcbind
    systemctl start rpcbind
    
    # 스토리지 디렉토리 생성
    mkdir -p /export/kubernetes/{dynamic,static}
    mkdir -p /export/databases
    mkdir -p /export/logs
    mkdir -p /export/backups
    
    # 권한 설정
    chown -R nobody:nobody /export
    chmod -R 755 /export
    
    # exports 설정
    cat > /etc/exports << 'EOF'
# Kubernetes Storage
/export/kubernetes/dynamic 192.168.1.0/24(rw,sync,no_root_squash,no_subtree_check)
/export/kubernetes/static  192.168.1.0/24(rw,sync,no_root_squash,no_subtree_check)
/export/databases          192.168.1.0/24(rw,sync,no_root_squash,no_subtree_check)
/export/logs               192.168.1.0/24(rw,sync,no_root_squash,no_subtree_check)
/export/backups            192.168.1.0/24(rw,sync,no_root_squash,no_subtree_check)
EOF
    
    # exports 적용
    exportfs -ra
    
    # 방화벽 설정
    firewall-cmd --permanent --add-service=nfs
    firewall-cmd --permanent --add-service=mountd
    firewall-cmd --permanent --add-service=rpc-bind
    firewall-cmd --reload
    
    echo "NFS server setup completed"
}

# 고성능 NFS 설정
optimize_nfs_performance() {
    echo "Optimizing NFS performance..."
    
    # NFS 데몬 수 증가
    sed -i 's/#RPCNFSDCOUNT=.*/RPCNFSDCOUNT=16/' /etc/sysconfig/nfs
    
    # TCP 사용 강제
    echo "RPCMOUNTDOPTS=\"-p 892\"" >> /etc/sysconfig/nfs
    
    # 네트워크 최적화
    cat >> /etc/sysctl.conf << 'EOF'
# NFS optimization
net.core.rmem_default = 262144
net.core.rmem_max = 16777216
net.core.wmem_default = 262144
net.core.wmem_max = 16777216
net.ipv4.tcp_rmem = 4096 65536 16777216
net.ipv4.tcp_wmem = 4096 65536 16777216
EOF
    
    sysctl -p
    
    # NFS 서비스 재시작
    systemctl restart nfs-server
    
    echo "NFS performance optimization completed"
}

setup_nfs_server
optimize_nfs_performance
```

### NFS StorageClass 설정

```yaml
# nfs-storage-class.yaml
---
# NFS Subdir External Provisioner용 StorageClass
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-dynamic
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner
parameters:
  archiveOnDelete: "false"  # PVC 삭제 시 데이터 보관 여부
  pathPattern: "${.PVC.namespace}/${.PVC.name}"  # 디렉토리 패턴
  onDelete: delete  # retain, archive, delete
allowVolumeExpansion: true
volumeBindingMode: Immediate
reclaimPolicy: Delete

---
# 성능 최적화된 NFS StorageClass
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-performance
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner
parameters:
  archiveOnDelete: "false"
  pathPattern: "performance/${.PVC.namespace}/${.PVC.name}"
  mountOptions: "nfsvers=4.1,hard,rsize=1048576,wsize=1048576,timeo=600,retrans=2"
allowVolumeExpansion: true
volumeBindingMode: Immediate
reclaimPolicy: Retain

---
# 백업용 NFS StorageClass
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-backup
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner
parameters:
  archiveOnDelete: "true"
  pathPattern: "backups/${.PVC.namespace}/${.PVC.name}"
allowVolumeExpansion: true
volumeBindingMode: Immediate
reclaimPolicy: Retain
```

## 🐙 Ceph 클러스터 구축

### Ceph 클러스터 설치

```bash
#!/bin/bash
# setup-ceph-cluster.sh

# Ceph 클러스터 구성 변수
CEPH_NODES=("ceph-01" "ceph-02" "ceph-03")
CEPH_PUBLIC_NETWORK="192.168.1.0/24"
CEPH_CLUSTER_NETWORK="192.168.2.0/24"

# Cephx 클러스터 초기화
initialize_ceph_cluster() {
    echo "Initializing Ceph cluster..."
    
    # cephadm 설치
    curl --silent --remote-name --location https://github.com/ceph/ceph/raw/quincy/src/cephadm/cephadm
    chmod +x cephadm
    ./cephadm install
    
    # 클러스터 부트스트랩
    cephadm bootstrap \
        --mon-ip 192.168.1.10 \
        --cluster-network $CEPH_CLUSTER_NETWORK \
        --ssh-user root \
        --ssh-private-key /root/.ssh/id_rsa \
        --skip-monitoring-stack
    
    # Ceph CLI 설치
    cephadm install ceph-common
    
    echo "Ceph cluster initialized"
}

# 추가 노드 등록
add_ceph_nodes() {
    echo "Adding Ceph nodes..."
    
    for node in "${CEPH_NODES[@]:1}"; do
        echo "Adding node: $node"
        
        # SSH 키 복사
        ssh-copy-id root@$node
        
        # 노드 추가
        ceph orch host add $node 192.168.1.$(echo $node | sed 's/ceph-0/1/')
        
        # MON 추가
        ceph orch apply mon --placement="$node"
        
        # MGR 추가
        ceph orch apply mgr --placement="$node"
    done
    
    echo "Ceph nodes added"
}

# OSD 생성
create_osds() {
    echo "Creating OSDs..."
    
    # 모든 사용 가능한 디스크에 OSD 생성
    ceph orch apply osd --all-available-devices
    
    # 또는 특정 디스크 지정
    # for node in "${CEPH_NODES[@]}"; do
    #     ceph orch daemon add osd $node:/dev/sdb
    #     ceph orch daemon add osd $node:/dev/sdc
    # done
    
    echo "OSDs created"
}

# RBD 풀 생성
create_rbd_pools() {
    echo "Creating RBD pools..."
    
    # Kubernetes용 RBD 풀
    ceph osd pool create kubernetes 64 64
    ceph osd pool application enable kubernetes rbd
    
    # 데이터베이스용 고성능 풀
    ceph osd pool create database-ssd 32 32
    ceph osd pool application enable database-ssd rbd
    
    # 일반 애플리케이션용 풀
    ceph osd pool create general 64 64
    ceph osd pool application enable general rbd
    
    echo "RBD pools created"
}

# 사용자 및 키 생성
create_ceph_users() {
    echo "Creating Ceph users..."
    
    # Kubernetes CSI 드라이버용 사용자
    ceph auth get-or-create client.kubernetes \
        mon 'profile rbd' \
        osd 'profile rbd pool=kubernetes, profile rbd pool=database-ssd, profile rbd pool=general' \
        mgr 'profile rbd pool=kubernetes, profile rbd pool=database-ssd, profile rbd pool=general'
    
    # 키 추출
    ceph auth get-key client.kubernetes > /tmp/ceph-kubernetes.key
    
    echo "Ceph users created"
}

initialize_ceph_cluster
add_ceph_nodes
create_osds
create_rbd_pools
create_ceph_users
```

### Ceph CSI 드라이버 설치

```yaml
# ceph-csi-rbd.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: csi-rbd-secret
  namespace: ceph-csi-rbd
stringData:
  userID: kubernetes
  userKey: AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==  # ceph auth get-key client.kubernetes
  encryptionPassphrase: test_passphrase

---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ceph-rbd
provisioner: rbd.csi.ceph.com
parameters:
  clusterID: b9127830-b0cc-4e34-aa47-9d1a2e9949a8  # ceph fsid
  pool: kubernetes
  imageFormat: "2"
  imageFeatures: layering
  
  csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
  csi.storage.k8s.io/provisioner-secret-namespace: ceph-csi-rbd
  csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret
  csi.storage.k8s.io/controller-expand-secret-namespace: ceph-csi-rbd
  csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
  csi.storage.k8s.io/node-stage-secret-namespace: ceph-csi-rbd
  
  # 암호화 활성화 (옵션)
  encrypted: "true"
  
allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: Immediate

---
# 고성능 SSD 풀용 StorageClass
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ceph-rbd-ssd
provisioner: rbd.csi.ceph.com
parameters:
  clusterID: b9127830-b0cc-4e34-aa47-9d1a2e9949a8
  pool: database-ssd
  imageFormat: "2"
  imageFeatures: layering
  
  csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
  csi.storage.k8s.io/provisioner-secret-namespace: ceph-csi-rbd
  csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret
  csi.storage.k8s.io/controller-expand-secret-namespace: ceph-csi-rbd
  csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
  csi.storage.k8s.io/node-stage-secret-namespace: ceph-csi-rbd
  
allowVolumeExpansion: true
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
```

## 🔗 Longhorn 분산 스토리지

### Longhorn 설치

```bash
#!/bin/bash
# install-longhorn.sh

# 사전 요구사항 확인
check_prerequisites() {
    echo "Checking Longhorn prerequisites..."
    
    # 각 노드에서 필수 패키지 확인
    for node in $(kubectl get nodes -o name | cut -d/ -f2); do
        echo "Checking node: $node"
        
        ssh $node "
            # open-iscsi 설치
            if ! rpm -q iscsi-initiator-utils; then
                yum install -y iscsi-initiator-utils
                systemctl enable iscsid
                systemctl start iscsid
            fi
            
            # NFSv4 클라이언트 설치
            if ! rpm -q nfs-utils; then
                yum install -y nfs-utils
            fi
            
            # 디스크 공간 확인
            df -h /var/lib/longhorn
        "
    done
}

# Longhorn 네임스페이스 및 설치
install_longhorn() {
    echo "Installing Longhorn..."
    
    # 네임스페이스 생성
    kubectl create namespace longhorn-system
    
    # Longhorn 설치
    kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.4.0/deploy/longhorn.yaml
    
    # 설치 상태 확인
    echo "Waiting for Longhorn to be ready..."
    kubectl -n longhorn-system rollout status deployment/longhorn-manager
    kubectl -n longhorn-system rollout status deployment/longhorn-driver-deployer
    
    echo "Longhorn installed successfully"
}

# Longhorn UI 접근 설정
setup_longhorn_ui() {
    echo "Setting up Longhorn UI access..."
    
    # Ingress 생성
    cat << 'EOF' | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: longhorn-ingress
  namespace: longhorn-system
  annotations:
    nginx.ingress.kubernetes.io/auth-type: basic
    nginx.ingress.kubernetes.io/auth-secret: basic-auth
    nginx.ingress.kubernetes.io/auth-realm: 'Authentication Required'
spec:
  ingressClassName: nginx
  rules:
  - host: longhorn.k8s.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: longhorn-frontend
            port:
              number: 80
EOF
    
    # 기본 인증 설정
    htpasswd -c auth admin
    kubectl -n longhorn-system create secret generic basic-auth --from-file=auth
    
    echo "Longhorn UI available at: http://longhorn.k8s.local"
}

check_prerequisites
install_longhorn
setup_longhorn_ui
```

### Longhorn StorageClass 설정

```yaml
# longhorn-storage-classes.yaml
---
# 기본 Longhorn StorageClass
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: driver.longhorn.io
allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: Immediate
parameters:
  numberOfReplicas: "3"
  staleReplicaTimeout: "2880"
  fromBackup: ""
  fsType: "ext4"

---
# 고가용성 StorageClass (5개 복제본)
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-ha
provisioner: driver.longhorn.io
allowVolumeExpansion: true
reclaimPolicy: Retain
volumeBindingMode: Immediate
parameters:
  numberOfReplicas: "5"
  staleReplicaTimeout: "2880"
  fromBackup: ""
  fsType: "ext4"
  dataLocality: "best-effort"

---
# 단일 복제본 StorageClass (임시 데이터용)
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-single
provisioner: driver.longhorn.io
allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: Immediate
parameters:
  numberOfReplicas: "1"
  staleReplicaTimeout: "2880"
  fromBackup: ""
  fsType: "ext4"

---
# SSD 노드 전용 StorageClass
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-ssd
provisioner: driver.longhorn.io
allowVolumeExpansion: true
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
parameters:
  numberOfReplicas: "3"
  staleReplicaTimeout: "2880"
  fromBackup: ""
  fsType: "ext4"
  nodeSelector: "storage-type=ssd"
```

## 💽 Local Storage 최적화

### Local PV 설정

```yaml
# local-storage-setup.yaml
---
# Local Storage StorageClass
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Delete

---
# 고성능 데이터베이스용 Local PV
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-mysql-01
  labels:
    type: local
    performance: high
    database: mysql
spec:
  capacity:
    storage: 500Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage
  local:
    path: /mnt/ssd-storage/mysql-01
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - db-worker-01

---
# Redis용 Local PV
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-redis-01
  labels:
    type: local
    performance: high
    database: redis
spec:
  capacity:
    storage: 100Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage
  local:
    path: /mnt/nvme-storage/redis-01
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - cache-worker-01
```

### Local Storage Provisioner

```bash
#!/bin/bash
# setup-local-storage-provisioner.sh

# 디스크 준비 및 마운트
prepare_local_disks() {
    local node=$1
    local disk_device=$2
    local mount_point=$3
    
    echo "Preparing local disk on node $node..."
    
    ssh $node "
        # 파일시스템 생성
        mkfs.ext4 -F $disk_device
        
        # 마운트 포인트 생성
        mkdir -p $mount_point
        
        # fstab에 추가
        echo '$disk_device $mount_point ext4 defaults 0 2' >> /etc/fstab
        
        # 마운트
        mount $mount_point
        
        # 권한 설정
        chmod 755 $mount_point
        
        # 서브디렉토리 생성
        mkdir -p $mount_point/k8s-local-storage
    "
    
    echo "Local disk prepared on $node"
}

# Local Storage Provisioner 설치
install_local_storage_provisioner() {
    echo "Installing Local Storage Provisioner..."
    
    # Local Storage Provisioner 설치
    kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/sig-storage-local-static-provisioner/master/manifests/provisioner.yaml
    
    # ConfigMap 생성
    cat << 'EOF' | kubectl apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: local-provisioner-config
  namespace: local-storage
data:
  storageClassMap: |
    local-ssd:
       hostDir: /mnt/ssd-storage/k8s-local-storage
       mountDir: /mnt/ssd-storage/k8s-local-storage
       blockCleanerCommand:
         - "/scripts/shred.sh"
         - "2"
       volumeMode: Filesystem
       fsType: ext4
    local-nvme:
       hostDir: /mnt/nvme-storage/k8s-local-storage
       mountDir: /mnt/nvme-storage/k8s-local-storage
       blockCleanerCommand:
         - "/scripts/shred.sh"
         - "2"
       volumeMode: Filesystem
       fsType: ext4
EOF

    echo "Local Storage Provisioner installed"
}

# 노드별 디스크 설정 예제
# prepare_local_disks "db-worker-01" "/dev/sdb" "/mnt/ssd-storage"
# prepare_local_disks "cache-worker-01" "/dev/nvme0n1" "/mnt/nvme-storage"

install_local_storage_provisioner
```

## 📈 스토리지 성능 모니터링

### 스토리지 성능 측정

```bash
#!/bin/bash
# storage-performance-test.sh

# 순차 읽기/쓰기 성능 테스트
test_sequential_performance() {
    local test_dir=$1
    local test_size=${2:-1G}
    
    echo "Testing sequential performance on $test_dir..."
    
    # 순차 쓰기 테스트
    echo "Sequential write test:"
    dd if=/dev/zero of=$test_dir/seq_write_test bs=1M count=1024 oflag=direct 2>&1 | grep -E "(copied|GB/s|MB/s)"
    
    # 순차 읽기 테스트
    echo "Sequential read test:"
    dd if=$test_dir/seq_write_test of=/dev/null bs=1M iflag=direct 2>&1 | grep -E "(copied|GB/s|MB/s)"
    
    # 정리
    rm -f $test_dir/seq_write_test
}

# 랜덤 I/O 성능 테스트 (fio 사용)
test_random_io() {
    local test_dir=$1
    
    echo "Testing random I/O performance on $test_dir..."
    
    # 4K 랜덤 읽기 테스트
    fio --name=randread --ioengine=libaio --iodepth=32 --rw=randread \
        --bs=4k --direct=1 --size=1G --numjobs=4 --runtime=60 \
        --group_reporting --filename=$test_dir/fio_test
    
    # 4K 랜덤 쓰기 테스트
    fio --name=randwrite --ioengine=libaio --iodepth=32 --rw=randwrite \
        --bs=4k --direct=1 --size=1G --numjobs=4 --runtime=60 \
        --group_reporting --filename=$test_dir/fio_test
    
    # 정리
    rm -f $test_dir/fio_test
}

# 데이터베이스 시뮬레이션 테스트
test_database_workload() {
    local test_dir=$1
    
    echo "Testing database-like workload on $test_dir..."
    
    # 75% 읽기, 25% 쓰기 혼합 워크로드
    fio --name=database --ioengine=libaio --iodepth=64 --rw=randrw \
        --rwmixread=75 --bs=8k --direct=1 --size=2G --numjobs=8 \
        --runtime=300 --group_reporting --filename=$test_dir/db_test
    
    # 정리
    rm -f $test_dir/db_test
}

# 쿠버네티스 PVC 성능 테스트
test_pvc_performance() {
    local storage_class=$1
    local pvc_size=${2:-10Gi}
    
    echo "Testing PVC performance for storage class: $storage_class"
    
    # 테스트 PVC 생성
    cat << EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: performance-test-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: $storage_class
  resources:
    requests:
      storage: $pvc_size
EOF
    
    # Pod 생성 및 성능 테스트
    cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: storage-performance-test
spec:
  containers:
  - name: test-container
    image: ubuntu:20.04
    command: ["/bin/sh"]
    args: ["-c", "while true; do sleep 3600; done"]
    volumeMounts:
    - name: test-volume
      mountPath: /test-data
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
      limits:
        memory: "2Gi"
        cpu: "1000m"
  volumes:
  - name: test-volume
    persistentVolumeClaim:
      claimName: performance-test-pvc
EOF
    
    # Pod 준비 대기
    kubectl wait --for=condition=Ready pod/storage-performance-test --timeout=300s
    
    # 성능 테스트 실행
    kubectl exec storage-performance-test -- apt-get update
    kubectl exec storage-performance-test -- apt-get install -y fio
    
    echo "Running performance tests inside Pod..."
    kubectl exec storage-performance-test -- fio --name=test --ioengine=libaio \
        --iodepth=32 --rw=randread --bs=4k --direct=1 --size=1G \
        --runtime=60 --filename=/test-data/test-file
    
    # 정리
    kubectl delete pod storage-performance-test
    kubectl delete pvc performance-test-pvc
}

# 사용 예제
case "${1:-help}" in
    sequential)
        test_sequential_performance "$2"
        ;;
    random)
        test_random_io "$2"
        ;;
    database)
        test_database_workload "$2"
        ;;
    pvc)
        test_pvc_performance "$2" "$3"
        ;;
    *)
        echo "Usage: $0 {sequential|random|database|pvc} <path|storage-class> [size]"
        echo "Examples:"
        echo "  $0 sequential /mnt/ssd-storage"
        echo "  $0 random /mnt/nvme-storage"
        echo "  $0 database /test-dir"
        echo "  $0 pvc longhorn 20Gi"
        exit 1
        ;;
esac
```

## 🔧 스토리지 운영 관리

### 스토리지 상태 모니터링

```bash
#!/bin/bash
# storage-monitoring.sh

# 전체 스토리지 상태 확인
check_storage_overview() {
    echo "=== Storage Overview ==="
    echo "Timestamp: $(date)"
    echo
    
    # StorageClass 목록
    echo "=== Available Storage Classes ==="
    kubectl get storageclass -o custom-columns="NAME:.metadata.name,PROVISIONER:.provisioner,RECLAIM:.reclaimPolicy,BINDING:.volumeBindingMode,DEFAULT:.metadata.annotations.storageclass\.kubernetes\.io/is-default-class"
    echo
    
    # PV 상태 요약
    echo "=== PV Status Summary ==="
    kubectl get pv --no-headers | awk '{print $5}' | sort | uniq -c
    echo
    
    # PVC 상태 요약
    echo "=== PVC Status Summary ==="
    kubectl get pvc --all-namespaces --no-headers | awk '{print $4}' | sort | uniq -c
    echo
    
    # 스토리지 용량 사용량
    echo "=== Storage Capacity Usage ==="
    kubectl get pv --no-headers -o custom-columns="NAME:.metadata.name,CAPACITY:.spec.capacity.storage,STATUS:.status.phase" | \
    grep Bound | awk '{total+=$2} END {print "Total Bound Storage: " total " (approximate)"}'
}

# 스토리지 성능 메트릭 수집
collect_storage_metrics() {
    echo "=== Storage Performance Metrics ==="
    
    # 각 노드의 디스크 사용량
    echo "Node Disk Usage:"
    for node in $(kubectl get nodes -o name | cut -d/ -f2); do
        echo "Node: $node"
        ssh $node "df -h | grep -E '(Filesystem|/dev/)' | grep -v tmpfs" 2>/dev/null || echo "  SSH access not available"
        echo
    done
    
    # I/O 통계 (가능한 경우)
    echo "I/O Statistics:"
    for node in $(kubectl get nodes -o name | cut -d/ -f2); do
        echo "Node: $node"
        ssh $node "iostat -x 1 1 | tail -n +4" 2>/dev/null || echo "  iostat not available"
        echo
    done
}

# 스토리지 문제 진단
diagnose_storage_issues() {
    echo "=== Storage Issues Diagnosis ==="
    
    # Pending PVC 확인
    echo "Pending PVCs:"
    kubectl get pvc --all-namespaces --field-selector status.phase=Pending
    echo
    
    # Failed PV 확인
    echo "Failed PVs:"
    kubectl get pv --field-selector status.phase=Failed
    echo
    
    # 스토리지 관련 이벤트
    echo "Recent Storage Events:"
    kubectl get events --all-namespaces --sort-by='.lastTimestamp' | \
    grep -E "(FailedMount|FailedAttach|ProvisioningFailed|VolumeBinding)" | tail -10
    echo
    
    # CSI 드라이버 상태
    echo "CSI Driver Status:"
    kubectl get pods -n kube-system | grep csi
}

case "${1:-overview}" in
    overview)
        check_storage_overview
        ;;
    metrics)
        collect_storage_metrics
        ;;
    diagnose)
        diagnose_storage_issues
        ;;
    all)
        check_storage_overview
        echo
        collect_storage_metrics
        echo
        diagnose_storage_issues
        ;;
    *)
        echo "Usage: $0 {overview|metrics|diagnose|all}"
        exit 1
        ;;
esac
```

## 📋 스토리지 선택 가이드

### 워크로드별 권장 스토리지

| 워크로드 유형 | 권장 스토리지 | 이유 |
|---------------|---------------|------|
| **데이터베이스** | Local SSD > Ceph RBD SSD | 낮은 지연시간, 높은 IOPS |
| **웹 애플리케이션** | NFS > Longhorn | 다중 접근, 간편한 관리 |
| **로그 수집** | NFS > GlusterFS | 대용량, 순차 쓰기 |
| **캐시 (Redis)** | Local NVMe > Local SSD | 최고 성능 |
| **백업 저장소** | NFS > Ceph RBD | 비용 효율성, 대용량 |
| **CI/CD 빌드** | Local SSD > Longhorn | 임시 고성능 스토리지 |

### 환경별 권장 구성

**소규모 환경 (< 10 노드)**
- Primary: NFS + Local SSD
- 장점: 구성 간단, 비용 효율적
- 단점: 확장성 제한

**중규모 환경 (10-50 노드)**
- Primary: Longhorn + Local SSD
- 장점: 관리 편의성, 적당한 확장성
- 단점: 네트워크 오버헤드

**대규모 환경 (50+ 노드)**
- Primary: Ceph + Local SSD
- 장점: 높은 확장성, 고가용성
- 단점: 운영 복잡성

---

> 💡 **실전 경험**: 온프렘 스토리지 선택에서 가장 중요한 것은 운영팀의 전문성과 요구사항입니다. Ceph는 강력하지만 운영이 복잡하고, NFS는 간단하지만 성능에 제약이 있습니다. 처음에는 단순한 솔루션으로 시작해서 점진적으로 고도화하는 것을 권장합니다.

태그: #storage #onprem #nfs #ceph #longhorn #performance
