# ì˜¨í”„ë ˜ ìŠ¤í† ë¦¬ì§€ ì†”ë£¨ì…˜

## ğŸ¯ ê°œìš”

ì˜¨í”„ë ˜ ì¿ ë²„ë„¤í‹°ìŠ¤ í™˜ê²½ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ìŠ¤í† ë¦¬ì§€ ì†”ë£¨ì…˜ë“¤ì˜ íŠ¹ì§•, êµ¬ì¶• ë°©ë²•, ì„±ëŠ¥ ìµœì í™”, ê·¸ë¦¬ê³  ì‹¤ì „ ìš´ì˜ ê²½í—˜ì„ ë‹¤ë£¹ë‹ˆë‹¤.

## ğŸ“Š ìŠ¤í† ë¦¬ì§€ ì†”ë£¨ì…˜ ë¹„êµ

### ì£¼ìš” ì˜¨í”„ë ˜ ìŠ¤í† ë¦¬ì§€ ì˜µì…˜

| ì†”ë£¨ì…˜ | íƒ€ì… | ì„±ëŠ¥ | í™•ì¥ì„± | ë³µì¡ë„ | ê°€ìš©ì„± | ë¹„ìš© |
|--------|------|------|--------|--------|--------|------|
| **NFS** | Network | ì¤‘ê°„ | ì œí•œì  | ë‚®ìŒ | ì¤‘ê°„ | ë‚®ìŒ |
| **Ceph** | Distributed | ë†’ìŒ | ìš°ìˆ˜ | ë†’ìŒ | ë†’ìŒ | ì¤‘ê°„ |
| **GlusterFS** | Distributed | ì¤‘ê°„ | ìš°ìˆ˜ | ì¤‘ê°„ | ë†’ìŒ | ë‚®ìŒ |
| **Longhorn** | Cloud-Native | ì¤‘ê°„ | ìš°ìˆ˜ | ë‚®ìŒ | ë†’ìŒ | ë‚®ìŒ |
| **Local Storage** | Local | ë§¤ìš°ë†’ìŒ | ì—†ìŒ | ë‚®ìŒ | ë‚®ìŒ | ë‚®ìŒ |
| **iSCSI** | Network | ë†’ìŒ | ì¤‘ê°„ | ì¤‘ê°„ | ì¤‘ê°„ | ì¤‘ê°„ |

## ğŸ—„ï¸ NFS ìŠ¤í† ë¦¬ì§€ êµ¬ì„±

### NFS ì„œë²„ ì„¤ì •

```bash
#!/bin/bash
# setup-nfs-server.sh

# NFS ì„œë²„ ì„¤ì¹˜ ë° ì„¤ì • (CentOS/RHEL)
setup_nfs_server() {
    echo "Setting up NFS server..."
    
    # NFS íŒ¨í‚¤ì§€ ì„¤ì¹˜
    yum install -y nfs-utils nfs-utils-lib
    
    # NFS ì„œë¹„ìŠ¤ ì‹œì‘
    systemctl enable nfs-server
    systemctl start nfs-server
    systemctl enable rpcbind
    systemctl start rpcbind
    
    # ìŠ¤í† ë¦¬ì§€ ë””ë ‰í† ë¦¬ ìƒì„±
    mkdir -p /export/kubernetes/{dynamic,static}
    mkdir -p /export/databases
    mkdir -p /export/logs
    mkdir -p /export/backups
    
    # ê¶Œí•œ ì„¤ì •
    chown -R nobody:nobody /export
    chmod -R 755 /export
    
    # exports ì„¤ì •
    cat > /etc/exports << 'EOF'
# Kubernetes Storage
/export/kubernetes/dynamic 192.168.1.0/24(rw,sync,no_root_squash,no_subtree_check)
/export/kubernetes/static  192.168.1.0/24(rw,sync,no_root_squash,no_subtree_check)
/export/databases          192.168.1.0/24(rw,sync,no_root_squash,no_subtree_check)
/export/logs               192.168.1.0/24(rw,sync,no_root_squash,no_subtree_check)
/export/backups            192.168.1.0/24(rw,sync,no_root_squash,no_subtree_check)
EOF
    
    # exports ì ìš©
    exportfs -ra
    
    # ë°©í™”ë²½ ì„¤ì •
    firewall-cmd --permanent --add-service=nfs
    firewall-cmd --permanent --add-service=mountd
    firewall-cmd --permanent --add-service=rpc-bind
    firewall-cmd --reload
    
    echo "NFS server setup completed"
}

# ê³ ì„±ëŠ¥ NFS ì„¤ì •
optimize_nfs_performance() {
    echo "Optimizing NFS performance..."
    
    # NFS ë°ëª¬ ìˆ˜ ì¦ê°€
    sed -i 's/#RPCNFSDCOUNT=.*/RPCNFSDCOUNT=16/' /etc/sysconfig/nfs
    
    # TCP ì‚¬ìš© ê°•ì œ
    echo "RPCMOUNTDOPTS=\"-p 892\"" >> /etc/sysconfig/nfs
    
    # ë„¤íŠ¸ì›Œí¬ ìµœì í™”
    cat >> /etc/sysctl.conf << 'EOF'
# NFS optimization
net.core.rmem_default = 262144
net.core.rmem_max = 16777216
net.core.wmem_default = 262144
net.core.wmem_max = 16777216
net.ipv4.tcp_rmem = 4096 65536 16777216
net.ipv4.tcp_wmem = 4096 65536 16777216
EOF
    
    sysctl -p
    
    # NFS ì„œë¹„ìŠ¤ ì¬ì‹œì‘
    systemctl restart nfs-server
    
    echo "NFS performance optimization completed"
}

setup_nfs_server
optimize_nfs_performance
```

### NFS StorageClass ì„¤ì •

```yaml
# nfs-storage-class.yaml
---
# NFS Subdir External Provisionerìš© StorageClass
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-dynamic
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner
parameters:
  archiveOnDelete: "false"  # PVC ì‚­ì œ ì‹œ ë°ì´í„° ë³´ê´€ ì—¬ë¶€
  pathPattern: "${.PVC.namespace}/${.PVC.name}"  # ë””ë ‰í† ë¦¬ íŒ¨í„´
  onDelete: delete  # retain, archive, delete
allowVolumeExpansion: true
volumeBindingMode: Immediate
reclaimPolicy: Delete

---
# ì„±ëŠ¥ ìµœì í™”ëœ NFS StorageClass
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-performance
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner
parameters:
  archiveOnDelete: "false"
  pathPattern: "performance/${.PVC.namespace}/${.PVC.name}"
  mountOptions: "nfsvers=4.1,hard,rsize=1048576,wsize=1048576,timeo=600,retrans=2"
allowVolumeExpansion: true
volumeBindingMode: Immediate
reclaimPolicy: Retain

---
# ë°±ì—…ìš© NFS StorageClass
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-backup
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner
parameters:
  archiveOnDelete: "true"
  pathPattern: "backups/${.PVC.namespace}/${.PVC.name}"
allowVolumeExpansion: true
volumeBindingMode: Immediate
reclaimPolicy: Retain
```

## ğŸ™ Ceph í´ëŸ¬ìŠ¤í„° êµ¬ì¶•

### Ceph í´ëŸ¬ìŠ¤í„° ì„¤ì¹˜

```bash
#!/bin/bash
# setup-ceph-cluster.sh

# Ceph í´ëŸ¬ìŠ¤í„° êµ¬ì„± ë³€ìˆ˜
CEPH_NODES=("ceph-01" "ceph-02" "ceph-03")
CEPH_PUBLIC_NETWORK="192.168.1.0/24"
CEPH_CLUSTER_NETWORK="192.168.2.0/24"

# Cephx í´ëŸ¬ìŠ¤í„° ì´ˆê¸°í™”
initialize_ceph_cluster() {
    echo "Initializing Ceph cluster..."
    
    # cephadm ì„¤ì¹˜
    curl --silent --remote-name --location https://github.com/ceph/ceph/raw/quincy/src/cephadm/cephadm
    chmod +x cephadm
    ./cephadm install
    
    # í´ëŸ¬ìŠ¤í„° ë¶€íŠ¸ìŠ¤íŠ¸ë©
    cephadm bootstrap \
        --mon-ip 192.168.1.10 \
        --cluster-network $CEPH_CLUSTER_NETWORK \
        --ssh-user root \
        --ssh-private-key /root/.ssh/id_rsa \
        --skip-monitoring-stack
    
    # Ceph CLI ì„¤ì¹˜
    cephadm install ceph-common
    
    echo "Ceph cluster initialized"
}

# ì¶”ê°€ ë…¸ë“œ ë“±ë¡
add_ceph_nodes() {
    echo "Adding Ceph nodes..."
    
    for node in "${CEPH_NODES[@]:1}"; do
        echo "Adding node: $node"
        
        # SSH í‚¤ ë³µì‚¬
        ssh-copy-id root@$node
        
        # ë…¸ë“œ ì¶”ê°€
        ceph orch host add $node 192.168.1.$(echo $node | sed 's/ceph-0/1/')
        
        # MON ì¶”ê°€
        ceph orch apply mon --placement="$node"
        
        # MGR ì¶”ê°€
        ceph orch apply mgr --placement="$node"
    done
    
    echo "Ceph nodes added"
}

# OSD ìƒì„±
create_osds() {
    echo "Creating OSDs..."
    
    # ëª¨ë“  ì‚¬ìš© ê°€ëŠ¥í•œ ë””ìŠ¤í¬ì— OSD ìƒì„±
    ceph orch apply osd --all-available-devices
    
    # ë˜ëŠ” íŠ¹ì • ë””ìŠ¤í¬ ì§€ì •
    # for node in "${CEPH_NODES[@]}"; do
    #     ceph orch daemon add osd $node:/dev/sdb
    #     ceph orch daemon add osd $node:/dev/sdc
    # done
    
    echo "OSDs created"
}

# RBD í’€ ìƒì„±
create_rbd_pools() {
    echo "Creating RBD pools..."
    
    # Kubernetesìš© RBD í’€
    ceph osd pool create kubernetes 64 64
    ceph osd pool application enable kubernetes rbd
    
    # ë°ì´í„°ë² ì´ìŠ¤ìš© ê³ ì„±ëŠ¥ í’€
    ceph osd pool create database-ssd 32 32
    ceph osd pool application enable database-ssd rbd
    
    # ì¼ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ìš© í’€
    ceph osd pool create general 64 64
    ceph osd pool application enable general rbd
    
    echo "RBD pools created"
}

# ì‚¬ìš©ì ë° í‚¤ ìƒì„±
create_ceph_users() {
    echo "Creating Ceph users..."
    
    # Kubernetes CSI ë“œë¼ì´ë²„ìš© ì‚¬ìš©ì
    ceph auth get-or-create client.kubernetes \
        mon 'profile rbd' \
        osd 'profile rbd pool=kubernetes, profile rbd pool=database-ssd, profile rbd pool=general' \
        mgr 'profile rbd pool=kubernetes, profile rbd pool=database-ssd, profile rbd pool=general'
    
    # í‚¤ ì¶”ì¶œ
    ceph auth get-key client.kubernetes > /tmp/ceph-kubernetes.key
    
    echo "Ceph users created"
}

initialize_ceph_cluster
add_ceph_nodes
create_osds
create_rbd_pools
create_ceph_users
```

### Ceph CSI ë“œë¼ì´ë²„ ì„¤ì¹˜

```yaml
# ceph-csi-rbd.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: csi-rbd-secret
  namespace: ceph-csi-rbd
stringData:
  userID: kubernetes
  userKey: AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==  # ceph auth get-key client.kubernetes
  encryptionPassphrase: test_passphrase

---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ceph-rbd
provisioner: rbd.csi.ceph.com
parameters:
  clusterID: b9127830-b0cc-4e34-aa47-9d1a2e9949a8  # ceph fsid
  pool: kubernetes
  imageFormat: "2"
  imageFeatures: layering
  
  csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
  csi.storage.k8s.io/provisioner-secret-namespace: ceph-csi-rbd
  csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret
  csi.storage.k8s.io/controller-expand-secret-namespace: ceph-csi-rbd
  csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
  csi.storage.k8s.io/node-stage-secret-namespace: ceph-csi-rbd
  
  # ì•”í˜¸í™” í™œì„±í™” (ì˜µì…˜)
  encrypted: "true"
  
allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: Immediate

---
# ê³ ì„±ëŠ¥ SSD í’€ìš© StorageClass
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ceph-rbd-ssd
provisioner: rbd.csi.ceph.com
parameters:
  clusterID: b9127830-b0cc-4e34-aa47-9d1a2e9949a8
  pool: database-ssd
  imageFormat: "2"
  imageFeatures: layering
  
  csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
  csi.storage.k8s.io/provisioner-secret-namespace: ceph-csi-rbd
  csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret
  csi.storage.k8s.io/controller-expand-secret-namespace: ceph-csi-rbd
  csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
  csi.storage.k8s.io/node-stage-secret-namespace: ceph-csi-rbd
  
allowVolumeExpansion: true
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
```

## ğŸ”— Longhorn ë¶„ì‚° ìŠ¤í† ë¦¬ì§€

### Longhorn ì„¤ì¹˜

```bash
#!/bin/bash
# install-longhorn.sh

# ì‚¬ì „ ìš”êµ¬ì‚¬í•­ í™•ì¸
check_prerequisites() {
    echo "Checking Longhorn prerequisites..."
    
    # ê° ë…¸ë“œì—ì„œ í•„ìˆ˜ íŒ¨í‚¤ì§€ í™•ì¸
    for node in $(kubectl get nodes -o name | cut -d/ -f2); do
        echo "Checking node: $node"
        
        ssh $node "
            # open-iscsi ì„¤ì¹˜
            if ! rpm -q iscsi-initiator-utils; then
                yum install -y iscsi-initiator-utils
                systemctl enable iscsid
                systemctl start iscsid
            fi
            
            # NFSv4 í´ë¼ì´ì–¸íŠ¸ ì„¤ì¹˜
            if ! rpm -q nfs-utils; then
                yum install -y nfs-utils
            fi
            
            # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
            df -h /var/lib/longhorn
        "
    done
}

# Longhorn ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë° ì„¤ì¹˜
install_longhorn() {
    echo "Installing Longhorn..."
    
    # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
    kubectl create namespace longhorn-system
    
    # Longhorn ì„¤ì¹˜
    kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.4.0/deploy/longhorn.yaml
    
    # ì„¤ì¹˜ ìƒíƒœ í™•ì¸
    echo "Waiting for Longhorn to be ready..."
    kubectl -n longhorn-system rollout status deployment/longhorn-manager
    kubectl -n longhorn-system rollout status deployment/longhorn-driver-deployer
    
    echo "Longhorn installed successfully"
}

# Longhorn UI ì ‘ê·¼ ì„¤ì •
setup_longhorn_ui() {
    echo "Setting up Longhorn UI access..."
    
    # Ingress ìƒì„±
    cat << 'EOF' | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: longhorn-ingress
  namespace: longhorn-system
  annotations:
    nginx.ingress.kubernetes.io/auth-type: basic
    nginx.ingress.kubernetes.io/auth-secret: basic-auth
    nginx.ingress.kubernetes.io/auth-realm: 'Authentication Required'
spec:
  ingressClassName: nginx
  rules:
  - host: longhorn.k8s.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: longhorn-frontend
            port:
              number: 80
EOF
    
    # ê¸°ë³¸ ì¸ì¦ ì„¤ì •
    htpasswd -c auth admin
    kubectl -n longhorn-system create secret generic basic-auth --from-file=auth
    
    echo "Longhorn UI available at: http://longhorn.k8s.local"
}

check_prerequisites
install_longhorn
setup_longhorn_ui
```

### Longhorn StorageClass ì„¤ì •

```yaml
# longhorn-storage-classes.yaml
---
# ê¸°ë³¸ Longhorn StorageClass
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: driver.longhorn.io
allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: Immediate
parameters:
  numberOfReplicas: "3"
  staleReplicaTimeout: "2880"
  fromBackup: ""
  fsType: "ext4"

---
# ê³ ê°€ìš©ì„± StorageClass (5ê°œ ë³µì œë³¸)
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-ha
provisioner: driver.longhorn.io
allowVolumeExpansion: true
reclaimPolicy: Retain
volumeBindingMode: Immediate
parameters:
  numberOfReplicas: "5"
  staleReplicaTimeout: "2880"
  fromBackup: ""
  fsType: "ext4"
  dataLocality: "best-effort"

---
# ë‹¨ì¼ ë³µì œë³¸ StorageClass (ì„ì‹œ ë°ì´í„°ìš©)
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-single
provisioner: driver.longhorn.io
allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: Immediate
parameters:
  numberOfReplicas: "1"
  staleReplicaTimeout: "2880"
  fromBackup: ""
  fsType: "ext4"

---
# SSD ë…¸ë“œ ì „ìš© StorageClass
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-ssd
provisioner: driver.longhorn.io
allowVolumeExpansion: true
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
parameters:
  numberOfReplicas: "3"
  staleReplicaTimeout: "2880"
  fromBackup: ""
  fsType: "ext4"
  nodeSelector: "storage-type=ssd"
```

## ğŸ’½ Local Storage ìµœì í™”

### Local PV ì„¤ì •

```yaml
# local-storage-setup.yaml
---
# Local Storage StorageClass
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Delete

---
# ê³ ì„±ëŠ¥ ë°ì´í„°ë² ì´ìŠ¤ìš© Local PV
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-mysql-01
  labels:
    type: local
    performance: high
    database: mysql
spec:
  capacity:
    storage: 500Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage
  local:
    path: /mnt/ssd-storage/mysql-01
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - db-worker-01

---
# Redisìš© Local PV
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-redis-01
  labels:
    type: local
    performance: high
    database: redis
spec:
  capacity:
    storage: 100Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage
  local:
    path: /mnt/nvme-storage/redis-01
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - cache-worker-01
```

### Local Storage Provisioner

```bash
#!/bin/bash
# setup-local-storage-provisioner.sh

# ë””ìŠ¤í¬ ì¤€ë¹„ ë° ë§ˆìš´íŠ¸
prepare_local_disks() {
    local node=$1
    local disk_device=$2
    local mount_point=$3
    
    echo "Preparing local disk on node $node..."
    
    ssh $node "
        # íŒŒì¼ì‹œìŠ¤í…œ ìƒì„±
        mkfs.ext4 -F $disk_device
        
        # ë§ˆìš´íŠ¸ í¬ì¸íŠ¸ ìƒì„±
        mkdir -p $mount_point
        
        # fstabì— ì¶”ê°€
        echo '$disk_device $mount_point ext4 defaults 0 2' >> /etc/fstab
        
        # ë§ˆìš´íŠ¸
        mount $mount_point
        
        # ê¶Œí•œ ì„¤ì •
        chmod 755 $mount_point
        
        # ì„œë¸Œë””ë ‰í† ë¦¬ ìƒì„±
        mkdir -p $mount_point/k8s-local-storage
    "
    
    echo "Local disk prepared on $node"
}

# Local Storage Provisioner ì„¤ì¹˜
install_local_storage_provisioner() {
    echo "Installing Local Storage Provisioner..."
    
    # Local Storage Provisioner ì„¤ì¹˜
    kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/sig-storage-local-static-provisioner/master/manifests/provisioner.yaml
    
    # ConfigMap ìƒì„±
    cat << 'EOF' | kubectl apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: local-provisioner-config
  namespace: local-storage
data:
  storageClassMap: |
    local-ssd:
       hostDir: /mnt/ssd-storage/k8s-local-storage
       mountDir: /mnt/ssd-storage/k8s-local-storage
       blockCleanerCommand:
         - "/scripts/shred.sh"
         - "2"
       volumeMode: Filesystem
       fsType: ext4
    local-nvme:
       hostDir: /mnt/nvme-storage/k8s-local-storage
       mountDir: /mnt/nvme-storage/k8s-local-storage
       blockCleanerCommand:
         - "/scripts/shred.sh"
         - "2"
       volumeMode: Filesystem
       fsType: ext4
EOF

    echo "Local Storage Provisioner installed"
}

# ë…¸ë“œë³„ ë””ìŠ¤í¬ ì„¤ì • ì˜ˆì œ
# prepare_local_disks "db-worker-01" "/dev/sdb" "/mnt/ssd-storage"
# prepare_local_disks "cache-worker-01" "/dev/nvme0n1" "/mnt/nvme-storage"

install_local_storage_provisioner
```

## ğŸ“ˆ ìŠ¤í† ë¦¬ì§€ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### ìŠ¤í† ë¦¬ì§€ ì„±ëŠ¥ ì¸¡ì •

```bash
#!/bin/bash
# storage-performance-test.sh

# ìˆœì°¨ ì½ê¸°/ì“°ê¸° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
test_sequential_performance() {
    local test_dir=$1
    local test_size=${2:-1G}
    
    echo "Testing sequential performance on $test_dir..."
    
    # ìˆœì°¨ ì“°ê¸° í…ŒìŠ¤íŠ¸
    echo "Sequential write test:"
    dd if=/dev/zero of=$test_dir/seq_write_test bs=1M count=1024 oflag=direct 2>&1 | grep -E "(copied|GB/s|MB/s)"
    
    # ìˆœì°¨ ì½ê¸° í…ŒìŠ¤íŠ¸
    echo "Sequential read test:"
    dd if=$test_dir/seq_write_test of=/dev/null bs=1M iflag=direct 2>&1 | grep -E "(copied|GB/s|MB/s)"
    
    # ì •ë¦¬
    rm -f $test_dir/seq_write_test
}

# ëœë¤ I/O ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (fio ì‚¬ìš©)
test_random_io() {
    local test_dir=$1
    
    echo "Testing random I/O performance on $test_dir..."
    
    # 4K ëœë¤ ì½ê¸° í…ŒìŠ¤íŠ¸
    fio --name=randread --ioengine=libaio --iodepth=32 --rw=randread \
        --bs=4k --direct=1 --size=1G --numjobs=4 --runtime=60 \
        --group_reporting --filename=$test_dir/fio_test
    
    # 4K ëœë¤ ì“°ê¸° í…ŒìŠ¤íŠ¸
    fio --name=randwrite --ioengine=libaio --iodepth=32 --rw=randwrite \
        --bs=4k --direct=1 --size=1G --numjobs=4 --runtime=60 \
        --group_reporting --filename=$test_dir/fio_test
    
    # ì •ë¦¬
    rm -f $test_dir/fio_test
}

# ë°ì´í„°ë² ì´ìŠ¤ ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸
test_database_workload() {
    local test_dir=$1
    
    echo "Testing database-like workload on $test_dir..."
    
    # 75% ì½ê¸°, 25% ì“°ê¸° í˜¼í•© ì›Œí¬ë¡œë“œ
    fio --name=database --ioengine=libaio --iodepth=64 --rw=randrw \
        --rwmixread=75 --bs=8k --direct=1 --size=2G --numjobs=8 \
        --runtime=300 --group_reporting --filename=$test_dir/db_test
    
    # ì •ë¦¬
    rm -f $test_dir/db_test
}

# ì¿ ë²„ë„¤í‹°ìŠ¤ PVC ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
test_pvc_performance() {
    local storage_class=$1
    local pvc_size=${2:-10Gi}
    
    echo "Testing PVC performance for storage class: $storage_class"
    
    # í…ŒìŠ¤íŠ¸ PVC ìƒì„±
    cat << EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: performance-test-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: $storage_class
  resources:
    requests:
      storage: $pvc_size
EOF
    
    # Pod ìƒì„± ë° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: storage-performance-test
spec:
  containers:
  - name: test-container
    image: ubuntu:20.04
    command: ["/bin/sh"]
    args: ["-c", "while true; do sleep 3600; done"]
    volumeMounts:
    - name: test-volume
      mountPath: /test-data
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
      limits:
        memory: "2Gi"
        cpu: "1000m"
  volumes:
  - name: test-volume
    persistentVolumeClaim:
      claimName: performance-test-pvc
EOF
    
    # Pod ì¤€ë¹„ ëŒ€ê¸°
    kubectl wait --for=condition=Ready pod/storage-performance-test --timeout=300s
    
    # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    kubectl exec storage-performance-test -- apt-get update
    kubectl exec storage-performance-test -- apt-get install -y fio
    
    echo "Running performance tests inside Pod..."
    kubectl exec storage-performance-test -- fio --name=test --ioengine=libaio \
        --iodepth=32 --rw=randread --bs=4k --direct=1 --size=1G \
        --runtime=60 --filename=/test-data/test-file
    
    # ì •ë¦¬
    kubectl delete pod storage-performance-test
    kubectl delete pvc performance-test-pvc
}

# ì‚¬ìš© ì˜ˆì œ
case "${1:-help}" in
    sequential)
        test_sequential_performance "$2"
        ;;
    random)
        test_random_io "$2"
        ;;
    database)
        test_database_workload "$2"
        ;;
    pvc)
        test_pvc_performance "$2" "$3"
        ;;
    *)
        echo "Usage: $0 {sequential|random|database|pvc} <path|storage-class> [size]"
        echo "Examples:"
        echo "  $0 sequential /mnt/ssd-storage"
        echo "  $0 random /mnt/nvme-storage"
        echo "  $0 database /test-dir"
        echo "  $0 pvc longhorn 20Gi"
        exit 1
        ;;
esac
```

## ğŸ”§ ìŠ¤í† ë¦¬ì§€ ìš´ì˜ ê´€ë¦¬

### ìŠ¤í† ë¦¬ì§€ ìƒíƒœ ëª¨ë‹ˆí„°ë§

```bash
#!/bin/bash
# storage-monitoring.sh

# ì „ì²´ ìŠ¤í† ë¦¬ì§€ ìƒíƒœ í™•ì¸
check_storage_overview() {
    echo "=== Storage Overview ==="
    echo "Timestamp: $(date)"
    echo
    
    # StorageClass ëª©ë¡
    echo "=== Available Storage Classes ==="
    kubectl get storageclass -o custom-columns="NAME:.metadata.name,PROVISIONER:.provisioner,RECLAIM:.reclaimPolicy,BINDING:.volumeBindingMode,DEFAULT:.metadata.annotations.storageclass\.kubernetes\.io/is-default-class"
    echo
    
    # PV ìƒíƒœ ìš”ì•½
    echo "=== PV Status Summary ==="
    kubectl get pv --no-headers | awk '{print $5}' | sort | uniq -c
    echo
    
    # PVC ìƒíƒœ ìš”ì•½
    echo "=== PVC Status Summary ==="
    kubectl get pvc --all-namespaces --no-headers | awk '{print $4}' | sort | uniq -c
    echo
    
    # ìŠ¤í† ë¦¬ì§€ ìš©ëŸ‰ ì‚¬ìš©ëŸ‰
    echo "=== Storage Capacity Usage ==="
    kubectl get pv --no-headers -o custom-columns="NAME:.metadata.name,CAPACITY:.spec.capacity.storage,STATUS:.status.phase" | \
    grep Bound | awk '{total+=$2} END {print "Total Bound Storage: " total " (approximate)"}'
}

# ìŠ¤í† ë¦¬ì§€ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
collect_storage_metrics() {
    echo "=== Storage Performance Metrics ==="
    
    # ê° ë…¸ë“œì˜ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰
    echo "Node Disk Usage:"
    for node in $(kubectl get nodes -o name | cut -d/ -f2); do
        echo "Node: $node"
        ssh $node "df -h | grep -E '(Filesystem|/dev/)' | grep -v tmpfs" 2>/dev/null || echo "  SSH access not available"
        echo
    done
    
    # I/O í†µê³„ (ê°€ëŠ¥í•œ ê²½ìš°)
    echo "I/O Statistics:"
    for node in $(kubectl get nodes -o name | cut -d/ -f2); do
        echo "Node: $node"
        ssh $node "iostat -x 1 1 | tail -n +4" 2>/dev/null || echo "  iostat not available"
        echo
    done
}

# ìŠ¤í† ë¦¬ì§€ ë¬¸ì œ ì§„ë‹¨
diagnose_storage_issues() {
    echo "=== Storage Issues Diagnosis ==="
    
    # Pending PVC í™•ì¸
    echo "Pending PVCs:"
    kubectl get pvc --all-namespaces --field-selector status.phase=Pending
    echo
    
    # Failed PV í™•ì¸
    echo "Failed PVs:"
    kubectl get pv --field-selector status.phase=Failed
    echo
    
    # ìŠ¤í† ë¦¬ì§€ ê´€ë ¨ ì´ë²¤íŠ¸
    echo "Recent Storage Events:"
    kubectl get events --all-namespaces --sort-by='.lastTimestamp' | \
    grep -E "(FailedMount|FailedAttach|ProvisioningFailed|VolumeBinding)" | tail -10
    echo
    
    # CSI ë“œë¼ì´ë²„ ìƒíƒœ
    echo "CSI Driver Status:"
    kubectl get pods -n kube-system | grep csi
}

case "${1:-overview}" in
    overview)
        check_storage_overview
        ;;
    metrics)
        collect_storage_metrics
        ;;
    diagnose)
        diagnose_storage_issues
        ;;
    all)
        check_storage_overview
        echo
        collect_storage_metrics
        echo
        diagnose_storage_issues
        ;;
    *)
        echo "Usage: $0 {overview|metrics|diagnose|all}"
        exit 1
        ;;
esac
```

## ğŸ“‹ ìŠ¤í† ë¦¬ì§€ ì„ íƒ ê°€ì´ë“œ

### ì›Œí¬ë¡œë“œë³„ ê¶Œì¥ ìŠ¤í† ë¦¬ì§€

| ì›Œí¬ë¡œë“œ ìœ í˜• | ê¶Œì¥ ìŠ¤í† ë¦¬ì§€ | ì´ìœ  |
|---------------|---------------|------|
| **ë°ì´í„°ë² ì´ìŠ¤** | Local SSD > Ceph RBD SSD | ë‚®ì€ ì§€ì—°ì‹œê°„, ë†’ì€ IOPS |
| **ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜** | NFS > Longhorn | ë‹¤ì¤‘ ì ‘ê·¼, ê°„í¸í•œ ê´€ë¦¬ |
| **ë¡œê·¸ ìˆ˜ì§‘** | NFS > GlusterFS | ëŒ€ìš©ëŸ‰, ìˆœì°¨ ì“°ê¸° |
| **ìºì‹œ (Redis)** | Local NVMe > Local SSD | ìµœê³  ì„±ëŠ¥ |
| **ë°±ì—… ì €ì¥ì†Œ** | NFS > Ceph RBD | ë¹„ìš© íš¨ìœ¨ì„±, ëŒ€ìš©ëŸ‰ |
| **CI/CD ë¹Œë“œ** | Local SSD > Longhorn | ì„ì‹œ ê³ ì„±ëŠ¥ ìŠ¤í† ë¦¬ì§€ |

### í™˜ê²½ë³„ ê¶Œì¥ êµ¬ì„±

**ì†Œê·œëª¨ í™˜ê²½ (< 10 ë…¸ë“œ)**
- Primary: NFS + Local SSD
- ì¥ì : êµ¬ì„± ê°„ë‹¨, ë¹„ìš© íš¨ìœ¨ì 
- ë‹¨ì : í™•ì¥ì„± ì œí•œ

**ì¤‘ê·œëª¨ í™˜ê²½ (10-50 ë…¸ë“œ)**
- Primary: Longhorn + Local SSD
- ì¥ì : ê´€ë¦¬ í¸ì˜ì„±, ì ë‹¹í•œ í™•ì¥ì„±
- ë‹¨ì : ë„¤íŠ¸ì›Œí¬ ì˜¤ë²„í—¤ë“œ

**ëŒ€ê·œëª¨ í™˜ê²½ (50+ ë…¸ë“œ)**
- Primary: Ceph + Local SSD
- ì¥ì : ë†’ì€ í™•ì¥ì„±, ê³ ê°€ìš©ì„±
- ë‹¨ì : ìš´ì˜ ë³µì¡ì„±

---

> ğŸ’¡ **ì‹¤ì „ ê²½í—˜**: ì˜¨í”„ë ˜ ìŠ¤í† ë¦¬ì§€ ì„ íƒì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ ìš´ì˜íŒ€ì˜ ì „ë¬¸ì„±ê³¼ ìš”êµ¬ì‚¬í•­ì…ë‹ˆë‹¤. CephëŠ” ê°•ë ¥í•˜ì§€ë§Œ ìš´ì˜ì´ ë³µì¡í•˜ê³ , NFSëŠ” ê°„ë‹¨í•˜ì§€ë§Œ ì„±ëŠ¥ì— ì œì•½ì´ ìˆìŠµë‹ˆë‹¤. ì²˜ìŒì—ëŠ” ë‹¨ìˆœí•œ ì†”ë£¨ì…˜ìœ¼ë¡œ ì‹œì‘í•´ì„œ ì ì§„ì ìœ¼ë¡œ ê³ ë„í™”í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.

íƒœê·¸: #storage #onprem #nfs #ceph #longhorn #performance
