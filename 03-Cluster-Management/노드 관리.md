# 노드 관리

## 🎯 개요

쿠버네티스 클러스터에서 워커 노드와 마스터 노드의 생명주기 관리, 리소스 최적화, 문제 해결 방법을 다룹니다. 온프렘 환경에서의 실전 노드 운영 경험을 공유합니다.

## 🖥️ 노드 라이프사이클 관리

### 노드 상태 이해

```bash
# 노드 상태 확인
kubectl get nodes -o wide

# 노드 상세 정보
kubectl describe node <node-name>

# 노드 상태별 의미
# Ready: 노드가 Pod를 받을 준비가 됨
# NotReady: 노드에 문제가 있어 Pod를 스케줄링할 수 없음
# Unknown: 노드와 통신이 불가능한 상태
```

### 노드 추가

```bash
#!/bin/bash
# add-worker-node.sh

NODE_IP=$1
NODE_NAME=$2

if [ -z "$NODE_IP" ] || [ -z "$NODE_NAME" ]; then
    echo "Usage: $0 <node-ip> <node-name>"
    exit 1
fi

echo "Adding new worker node: $NODE_NAME ($NODE_IP)"

# 1. 새 노드에서 사전 준비
ssh root@$NODE_IP "
    # 호스트명 설정
    hostnamectl set-hostname $NODE_NAME
    
    # 필수 패키지 설치
    yum update -y
    yum install -y containerd.io kubelet kubeadm kubectl
    
    # 서비스 활성화
    systemctl enable containerd kubelet
    systemctl start containerd
    
    # 방화벽 설정
    firewall-cmd --permanent --add-port=10250/tcp
    firewall-cmd --permanent --add-port=30000-32767/tcp
    firewall-cmd --reload
    
    # swap 비활성화
    swapoff -a
    sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
"

# 2. join 토큰 생성 (마스터에서)
JOIN_COMMAND=$(kubeadm token create --print-join-command)

# 3. 새 노드에서 클러스터 조인
ssh root@$NODE_IP "$JOIN_COMMAND"

# 4. 노드 레이블링
kubectl label node $NODE_NAME node-role.kubernetes.io/worker=worker
kubectl label node $NODE_NAME environment=production

echo "Node $NODE_NAME added successfully!"
```

### 노드 제거

```bash
#!/bin/bash
# remove-node.sh

NODE_NAME=$1

if [ -z "$NODE_NAME" ]; then
    echo "Usage: $0 <node-name>"
    exit 1
fi

echo "Removing node: $NODE_NAME"

# 1. 노드 드레인 (Pod 이동)
echo "Draining node..."
kubectl drain $NODE_NAME --ignore-daemonsets --delete-emptydir-data --force --timeout=300s

# 2. 노드 삭제
echo "Deleting node from cluster..."
kubectl delete node $NODE_NAME

# 3. 노드에서 kubeadm 리셋 (선택사항)
read -p "Reset kubeadm on the node? (y/N): " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    NODE_IP=$(kubectl get node $NODE_NAME -o jsonpath='{.status.addresses[?(@.type=="InternalIP")].address}' 2>/dev/null || echo "")
    if [ -n "$NODE_IP" ]; then
        echo "Resetting kubeadm on $NODE_IP..."
        ssh root@$NODE_IP "
            kubeadm reset -f
            systemctl stop kubelet containerd
            rm -rf /etc/kubernetes/
            rm -rf /var/lib/kubelet/
            rm -rf /var/lib/etcd/
        "
    fi
fi

echo "Node $NODE_NAME removed successfully!"
```

## 🏷️ 노드 레이블링과 선택

### 표준 레이블링 전략

```bash
# 하드웨어 기반 레이블
kubectl label node worker-01 hardware.company.com/cpu-type=intel-xeon
kubectl label node worker-01 hardware.company.com/memory-size=32GB
kubectl label node worker-01 hardware.company.com/storage-type=ssd
kubectl label node worker-01 hardware.company.com/network-speed=10gbps

# 용도별 레이블
kubectl label node worker-01 workload.company.com/type=compute-intensive
kubectl label node worker-02 workload.company.com/type=memory-intensive
kubectl label node worker-03 workload.company.com/type=general-purpose

# 환경별 레이블
kubectl label node worker-01 environment=production
kubectl label node worker-02 environment=staging
kubectl label node worker-03 environment=development

# 지역/위치 레이블
kubectl label node worker-01 topology.kubernetes.io/zone=zone-a
kubectl label node worker-01 topology.kubernetes.io/region=korea-central

# 특수 용도 레이블
kubectl label node gpu-worker-01 accelerator.company.com/gpu=nvidia-v100
kubectl label node storage-worker-01 storage.company.com/type=ceph-osd
```

### Taints와 Tolerations

```bash
# 운영 환경 전용 노드 설정
kubectl taint nodes prod-worker-01 environment=production:NoSchedule

# 데이터베이스 전용 노드 설정
kubectl taint nodes db-worker-01 workload-type=database:NoSchedule

# GPU 전용 노드 설정
kubectl taint nodes gpu-worker-01 accelerator=gpu:NoSchedule

# 마스터 노드에 일반 Pod 스케줄링 허용 (개발 환경)
kubectl taint nodes master-01 node-role.kubernetes.io/control-plane:NoSchedule-

# Taint 제거
kubectl taint nodes worker-01 environment=production:NoSchedule-
```

### Pod 배치 전략

```yaml
# pod-placement-examples.yaml
---
# NodeSelector를 사용한 간단한 배치
apiVersion: v1
kind: Pod
metadata:
  name: web-app
spec:
  nodeSelector:
    workload.company.com/type: general-purpose
    environment: production
  containers:
  - name: web-app
    image: nginx

---
# Affinity를 사용한 고급 배치
apiVersion: apps/v1
kind: Deployment
metadata:
  name: database-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: database-app
  template:
    metadata:
      labels:
        app: database-app
    spec:
      # 선호하는 노드 배치
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: workload.company.com/type
                operator: In
                values:
                - memory-intensive
          - weight: 50
            preference:
              matchExpressions:
              - key: hardware.company.com/storage-type
                operator: In
                values:
                - ssd
        # Pod간 분산 배치
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - database-app
              topologyKey: kubernetes.io/hostname
      # Toleration 설정
      tolerations:
      - key: workload-type
        operator: Equal
        value: database
        effect: NoSchedule
      containers:
      - name: database
        image: postgres:13
```

## 📊 노드 리소스 관리

### 리소스 예약 설정

```yaml
# kubelet-config.yaml
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration

# 시스템 리소스 예약
systemReserved:
  cpu: "1000m"      # 1 CPU core
  memory: "2Gi"     # 2GB RAM
  ephemeral-storage: "10Gi"

# 쿠버네티스 컴포넌트 리소스 예약
kubeReserved:
  cpu: "500m"       # 0.5 CPU core
  memory: "1Gi"     # 1GB RAM
  ephemeral-storage: "5Gi"

# Eviction 임계값
evictionHard:
  memory.available: "500Mi"
  nodefs.available: "10%"
  nodefs.inodesFree: "5%"
  imagefs.available: "15%"

# Soft Eviction
evictionSoft:
  memory.available: "1Gi"
  nodefs.available: "15%"
evictionSoftGracePeriod:
  memory.available: "1m30s"
  nodefs.available: "2m"

# 이미지 가비지 컬렉션
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80

# 컨테이너 가비지 컬렉션
containerGCThreshold:
  minAge: 1m
  maxPerPodContainer: 2
  maxContainers: 240

# 최대 Pod 수 제한
maxPods: 250
```

### 리소스 모니터링

```bash
#!/bin/bash
# node-resource-monitor.sh

echo "=== Node Resource Monitor ==="
echo "Timestamp: $(date)"
echo

# 노드별 리소스 사용량
echo "=== Node Resource Usage ==="
kubectl top nodes --sort-by=cpu
echo

# 노드별 할당 가능한 리소스
echo "=== Node Allocatable Resources ==="
kubectl get nodes -o custom-columns="NAME:.metadata.name,CPU-ALLOC:.status.allocatable.cpu,MEM-ALLOC:.status.allocatable.memory,STORAGE-ALLOC:.status.allocatable.ephemeral-storage"
echo

# 노드별 실제 사용량 vs 요청량
echo "=== Resource Allocation vs Usage ==="
for node in $(kubectl get nodes -o name | cut -d/ -f2); do
    echo "Node: $node"
    
    # CPU 정보
    cpu_capacity=$(kubectl get node $node -o jsonpath='{.status.capacity.cpu}')
    cpu_allocatable=$(kubectl get node $node -o jsonpath='{.status.allocatable.cpu}')
    
    # 메모리 정보  
    mem_capacity=$(kubectl get node $node -o jsonpath='{.status.capacity.memory}')
    mem_allocatable=$(kubectl get node $node -o jsonpath='{.status.allocatable.memory}')
    
    echo "  CPU: $cpu_allocatable/$cpu_capacity"
    echo "  Memory: $mem_allocatable/$mem_capacity"
    
    # 현재 스케줄된 Pod의 리소스 요청량
    kubectl describe node $node | grep -A 5 "Allocated resources:"
    echo
done
```

## 🔧 노드 유지보수

### 계획된 유지보수

```bash
#!/bin/bash
# planned-maintenance.sh

NODE_NAME=$1
MAINTENANCE_REASON=${2:-"Planned maintenance"}

if [ -z "$NODE_NAME" ]; then
    echo "Usage: $0 <node-name> [reason]"
    exit 1
fi

echo "Starting planned maintenance for node: $NODE_NAME"
echo "Reason: $MAINTENANCE_REASON"

# 1. 유지보수 전 상태 저장
echo "Saving pre-maintenance state..."
kubectl get pods --all-namespaces --field-selector spec.nodeName=$NODE_NAME -o yaml > "/tmp/pre-maintenance-pods-$NODE_NAME.yaml"

# 2. 노드에 유지보수 레이블 추가
kubectl label node $NODE_NAME maintenance.company.com/status=in-progress
kubectl label node $NODE_NAME maintenance.company.com/reason="$MAINTENANCE_REASON"
kubectl label node $NODE_NAME maintenance.company.com/start-time="$(date -u +%Y-%m-%dT%H:%M:%SZ)"

# 3. 노드 드레인
echo "Draining node..."
kubectl drain $NODE_NAME \
    --ignore-daemonsets \
    --delete-emptydir-data \
    --timeout=600s \
    --grace-period=60

if [ $? -eq 0 ]; then
    echo "Node successfully drained"
    
    # 4. 실제 유지보수 작업 수행
    echo "Node is ready for maintenance"
    echo "Perform your maintenance tasks now..."
    echo "After completion, run: uncordon-node.sh $NODE_NAME"
else
    echo "Failed to drain node. Aborting maintenance."
    kubectl label node $NODE_NAME maintenance.company.com/status=failed
    exit 1
fi
```

### 유지보수 완료 후 복구

```bash
#!/bin/bash
# uncordon-node.sh

NODE_NAME=$1

if [ -z "$NODE_NAME" ]; then
    echo "Usage: $0 <node-name>"
    exit 1
fi

echo "Completing maintenance for node: $NODE_NAME"

# 1. 노드 uncordon
kubectl uncordon $NODE_NAME

# 2. 노드 상태 확인
echo "Waiting for node to become Ready..."
timeout=60
while [ $timeout -gt 0 ]; do
    node_status=$(kubectl get node $NODE_NAME --no-headers | awk '{print $2}')
    if [ "$node_status" = "Ready" ]; then
        echo "Node is Ready"
        break
    fi
    sleep 5
    timeout=$((timeout - 5))
done

if [ $timeout -le 0 ]; then
    echo "Warning: Node did not become Ready within 60 seconds"
fi

# 3. 유지보수 레이블 업데이트
kubectl label node $NODE_NAME maintenance.company.com/status=completed
kubectl label node $NODE_NAME maintenance.company.com/end-time="$(date -u +%Y-%m-%dT%H:%M:%SZ)"

# 4. Pod 재배치 확인
echo "Checking Pod redistribution..."
sleep 30
kubectl get pods --all-namespaces --field-selector spec.nodeName=$NODE_NAME

echo "Maintenance completed for node: $NODE_NAME"
```

### 노드 헬스체크

```bash
#!/bin/bash
# node-health-check.sh

NODE_NAME=$1

check_node_health() {
    local node=$1
    
    echo "=== Health Check for Node: $node ==="
    
    # 노드 기본 상태
    echo "Node Status:"
    kubectl get node $node -o wide
    echo
    
    # 노드 조건 확인
    echo "Node Conditions:"
    kubectl describe node $node | grep -A 20 "Conditions:"
    echo
    
    # 시스템 리소스 사용량
    echo "Resource Usage:"
    kubectl top node $node 2>/dev/null || echo "Metrics not available"
    echo
    
    # 노드에서 실행 중인 Pod 상태
    echo "Running Pods:"
    kubectl get pods --all-namespaces --field-selector spec.nodeName=$node
    echo
    
    # 디스크 사용량 (SSH 접근 가능한 경우)
    echo "Disk Usage (via SSH):"
    ssh $node "df -h | grep -E '(Filesystem|/dev/)'" 2>/dev/null || echo "SSH not available"
    echo
    
    # 시스템 서비스 상태
    echo "System Services (via SSH):"
    ssh $node "systemctl is-active kubelet containerd" 2>/dev/null || echo "SSH not available"
    echo
    
    # kubelet 로그 확인
    echo "Recent kubelet logs:"
    kubectl logs -n kube-system $(kubectl get pods -n kube-system -l component=kubelet -o name | head -1) --tail=10 2>/dev/null || echo "kubelet logs not available via kubectl"
}

if [ -z "$NODE_NAME" ]; then
    echo "Performing health check for all nodes..."
    for node in $(kubectl get nodes -o name | cut -d/ -f2); do
        check_node_health $node
        echo "================================"
    done
else
    check_node_health $NODE_NAME
fi
```

## 🔍 노드 트러블슈팅

### 일반적인 노드 문제

#### 1. NotReady 상태 해결

```bash
# 노드 상태 상세 확인
kubectl describe node <node-name>

# kubelet 상태 확인
ssh <node-name> "systemctl status kubelet"

# kubelet 로그 확인
ssh <node-name> "journalctl -u kubelet -f"

# 일반적인 해결 방법
ssh <node-name> "
    # 1. kubelet 재시작
    systemctl restart kubelet
    
    # 2. containerd 재시작
    systemctl restart containerd
    
    # 3. 디스크 공간 확인 및 정리
    df -h
    docker system prune -a
    
    # 4. 메모리 확인
    free -h
"
```

#### 2. Pod 스케줄링 실패

```bash
# 리소스 부족 확인
kubectl describe node <node-name> | grep -A 10 "Allocated resources:"

# Taint 확인
kubectl describe node <node-name> | grep Taints

# 노드 어피니티 확인
kubectl get pods <pod-name> -o yaml | grep -A 10 affinity
```

#### 3. 네트워크 문제

```bash
# CNI 플러그인 상태 확인
kubectl get pods -n kube-system | grep -E "calico|flannel|weave"

# 노드 간 네트워크 연결성 확인
ssh <node1> "ping -c 3 <node2-ip>"

# iptables 규칙 확인
ssh <node-name> "iptables -L -n | grep KUBE"
```

## 📊 노드 성능 최적화

### CPU 최적화

```bash
# CPU Governor 설정
ssh <node-name> "
    # 성능 모드로 설정
    echo performance | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
    
    # 현재 설정 확인
    cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor
"

# kubelet CPU 관리 정책
cat > /var/lib/kubelet/config.yaml << 'EOF'
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
cpuManagerPolicy: static
topologyManagerPolicy: single-numa-node
reservedSystemCPUs: "0,1"
EOF
```

### 메모리 최적화

```bash
# 스왑 완전 비활성화 확인
ssh <node-name> "
    swapoff -a
    sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
    cat /proc/swaps
"

# Huge Pages 설정 (필요한 경우)
ssh <node-name> "
    echo 1024 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
    echo 'vm.nr_hugepages = 1024' >> /etc/sysctl.conf
"
```

### 스토리지 최적화

```bash
# SSD 최적화 설정
ssh <node-name> "
    # 스케줄러 설정
    echo noop > /sys/block/sda/queue/scheduler
    
    # read-ahead 설정
    blockdev --setra 256 /dev/sda
    
    # 영구 설정을 위해 /etc/rc.local에 추가
"
```

## 📋 노드 관리 체크리스트

### 일일 점검 항목
- [ ] 모든 노드가 Ready 상태인가?
- [ ] CPU/메모리 사용률이 임계치 이하인가?
- [ ] 디스크 공간이 충분한가?
- [ ] kubelet 서비스가 정상 실행 중인가?

### 주간 점검 항목  
- [ ] 노드별 리소스 사용 트렌드 분석
- [ ] 시스템 업데이트 확인
- [ ] 로그 파일 크기 점검
- [ ] 네트워크 성능 확인

### 월간 점검 항목
- [ ] 하드웨어 상태 점검
- [ ] 성능 벤치마크 수행
- [ ] 용량 계획 검토
- [ ] 백업 및 복구 테스트

---

> 💡 **실전 경험**: 노드 관리의 핵심은 예방적 모니터링입니다. 리소스 사용량을 지속적으로 추적하고, 임계값에 도달하기 전에 조치를 취하세요. 특히 온프렘에서는 하드웨어 장애에 대비한 여분 노드 확보가 중요합니다.

태그: #node-management #maintenance #troubleshooting #scaling #onprem