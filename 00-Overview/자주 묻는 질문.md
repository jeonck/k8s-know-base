# ❓ 자주 묻는 질문 (FAQ)

> 온프렘 Kubernetes 운영 중 자주 발생하는 질문과 해결책

## 🏗️ 설치와 초기 설정

### Q1. kubeadm init 실행 시 "connection refused" 오류가 발생합니다

**A1.** 다음 사항들을 확인하세요:

```bash
# 1. containerd 상태 확인
sudo systemctl status containerd

# 2. kubelet 상태 확인
sudo systemctl status kubelet

# 3. 방화벽 설정 확인
sudo ufw status
# 또는 필요한 포트 오픈:
sudo ufw allow 6443/tcp
sudo ufw allow 2379:2380/tcp
sudo ufw allow 10250:10252/tcp

# 4. swap 비활성화 확인
free -h
sudo swapoff -a

# 5. 네트워크 설정 확인
ip route show
```

### Q2. CNI 플러그인 설치 후에도 노드가 NotReady 상태입니다

**A2.** CNI 설정을 확인하고 재설치하세요:

```bash
# 1. CNI Pod 상태 확인
kubectl get pods -n kube-system | grep -E "calico|flannel|weave"

# 2. CNI Pod 로그 확인
kubectl logs -n kube-system <cni-pod-name>

# 3. 네트워크 설정 확인
cat /etc/cni/net.d/*

# 4. CNI 재설치 (Calico 예시)
kubectl delete -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/tigera-operator.yaml
kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/tigera-operator.yaml
```

### Q3. 워커 노드 조인이 실패합니다

**A3.** 토큰과 네트워크 연결을 확인하세요:

```bash
# 1. 토큰 유효성 확인 (마스터 노드에서)
kubeadm token list

# 2. 새 토큰 생성 (필요시)
kubeadm token create --print-join-command

# 3. 네트워크 연결 확인 (워커 노드에서)
telnet <MASTER_IP> 6443

# 4. 인증서 해시 재확인
openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
```

## 🔧 클러스터 관리

### Q4. etcd 백업은 어떻게 자동화하나요?

**A4.** cron을 사용한 자동 백업 스크립트:

```bash
#!/bin/bash
# /usr/local/bin/etcd-backup.sh

BACKUP_DIR="/var/backups/etcd"
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="$BACKUP_DIR/etcd-snapshot-$DATE.db"

mkdir -p "$BACKUP_DIR"

ETCDCTL_API=3 etcdctl snapshot save "$BACKUP_FILE" \
    --endpoints=https://127.0.0.1:2379 \
    --cacert=/etc/kubernetes/pki/etcd/ca.crt \
    --cert=/etc/kubernetes/pki/etcd/server.crt \
    --key=/etc/kubernetes/pki/etcd/server.key

# 검증 및 압축
ETCDCTL_API=3 etcdctl snapshot status "$BACKUP_FILE"
gzip "$BACKUP_FILE"

# 30일 이상된 백업 삭제
find "$BACKUP_DIR" -name "etcd-snapshot-*.db.gz" -mtime +30 -delete

# crontab 설정: 매일 새벽 2시
# 0 2 * * * /usr/local/bin/etcd-backup.sh >> /var/log/etcd-backup.log 2>&1
```

### Q5. 클러스터 업그레이드는 어떻게 하나요?

**A5.** 단계별 업그레이드 절차:

```bash
# 1. 현재 버전 확인
kubectl version --short

# 2. 사용 가능한 버전 확인
apt update
apt-cache madison kubeadm

# 3. kubeadm 먼저 업그레이드 (마스터 노드)
apt-mark unhold kubeadm
apt-get update && apt-get install -y kubeadm=1.28.1-00
apt-mark hold kubeadm

# 4. 업그레이드 계획 확인
kubeadm upgrade plan

# 5. 마스터 노드 업그레이드
kubeadm upgrade apply v1.28.1

# 6. kubelet과 kubectl 업그레이드
apt-mark unhold kubelet kubectl
apt-get update && apt-get install -y kubelet=1.28.1-00 kubectl=1.28.1-00
apt-mark hold kubelet kubectl
sudo systemctl restart kubelet

# 7. 워커 노드들 순차적으로 업그레이드
# (각 워커 노드에서)
kubectl drain <worker-node> --ignore-daemonsets
# kubeadm과 kubelet 업그레이드 후
kubectl uncordon <worker-node>
```

### Q6. 노드가 갑자기 NotReady 상태가 되었습니다

**A6.** 단계별 진단 및 해결:

```bash
# 1. 노드 상태 상세 확인
kubectl describe node <node-name>

# 2. 노드 SSH 접속하여 확인
ssh <node-ip>

# 3. kubelet 상태 확인
sudo systemctl status kubelet
sudo journalctl -u kubelet -f

# 4. 컨테이너 런타임 확인
sudo systemctl status containerd
sudo crictl ps

# 5. 디스크 공간 확인
df -h
sudo du -sh /var/lib/kubelet/*

# 6. 메모리 확인
free -h

# 7. 복구 시도
sudo systemctl restart kubelet
sudo systemctl restart containerd
```

## 📦 애플리케이션 배포

### Q7. Pod가 ImagePullBackOff 상태입니다

**A7.** 이미지 관련 문제 해결:

```bash
# 1. Pod 상세 정보 확인
kubectl describe pod <pod-name>

# 2. 이미지 URL 확인
kubectl get pod <pod-name> -o jsonpath='{.spec.containers[0].image}'

# 3. 수동 이미지 풀 테스트 (해당 노드에서)
ssh <node-ip>
sudo crictl pull <image-name>

# 4. 레지스트리 인증 정보 확인
kubectl get secrets | grep docker
kubectl describe secret <registry-secret>

# 5. 프라이빗 레지스트리 시크릿 생성
kubectl create secret docker-registry regcred \
  --docker-server=<registry-url> \
  --docker-username=<username> \
  --docker-password=<password> \
  --docker-email=<email>

# 6. Pod에 imagePullSecrets 추가
kubectl patch deployment <deployment-name> -p '{"spec":{"template":{"spec":{"imagePullSecrets":[{"name":"regcred"}]}}}}'
```

### Q8. Pod가 Pending 상태에서 멈춰있습니다

**A8.** 스케줄링 문제 진단:

```bash
# 1. Pod 이벤트 확인
kubectl describe pod <pod-name>

# 2. 노드 리소스 확인
kubectl describe nodes

# 3. 리소스 사용량 확인
kubectl top nodes
kubectl top pods --all-namespaces

# 4. 타인트/톨러레이션 확인
kubectl get nodes -o json | jq '.items[] | {name: .metadata.name, taints: .spec.taints}'

# 5. 노드 셀렉터 확인
kubectl get pod <pod-name> -o yaml | grep -A 5 nodeSelector

# 6. PVC 상태 확인 (스토리지 문제인 경우)
kubectl get pvc
kubectl describe pvc <pvc-name>
```

### Q9. 서비스로 애플리케이션에 접근할 수 없습니다

**A9.** 네트워킹 문제 해결:

```bash
# 1. 서비스와 엔드포인트 확인
kubectl get svc
kubectl get endpoints <service-name>

# 2. Pod 라벨과 서비스 셀렉터 비교
kubectl get pods --show-labels
kubectl get svc <service-name> -o yaml | grep -A 3 selector

# 3. 네트워크 연결성 테스트
kubectl run test-pod --image=busybox --rm -it --restart=Never -- /bin/sh
# Pod 내에서:
# nslookup <service-name>
# wget -qO- <service-name>:<port>

# 4. kube-proxy 확인
kubectl get pods -n kube-system | grep kube-proxy
kubectl logs -n kube-system <kube-proxy-pod>

# 5. iptables 규칙 확인 (노드에서)
sudo iptables -L -n -t nat | grep <service-name>
```

## 🔒 보안과 권한

### Q10. RBAC 권한을 어떻게 설정하나요?

**A10.** 단계별 RBAC 설정:

```bash
# 1. 서비스 어카운트 생성
kubectl create serviceaccount <sa-name>

# 2. Role 생성 (네임스페이스 수준)
cat <<EOF | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
EOF

# 3. RoleBinding 생성
kubectl create rolebinding <binding-name> \
  --role=pod-reader \
  --serviceaccount=default:<sa-name>

# 4. 권한 테스트
kubectl auth can-i list pods --as=system:serviceaccount:default:<sa-name>

# 5. 클러스터 수준 권한 (ClusterRole/ClusterRoleBinding)
kubectl create clusterrole <role-name> --verb=get,list,watch --resource=nodes
kubectl create clusterrolebinding <binding-name> \
  --clusterrole=<role-name> \
  --serviceaccount=default:<sa-name>
```

### Q11. Pod 보안 정책을 어떻게 적용하나요?

**A11.** Pod Security Standards 적용:

```bash
# 1. 네임스페이스에 보안 정책 라벨 추가
kubectl label namespace <namespace> pod-security.kubernetes.io/enforce=restricted
kubectl label namespace <namespace> pod-security.kubernetes.io/audit=restricted
kubectl label namespace <namespace> pod-security.kubernetes.io/warn=restricted

# 2. 보안 컨텍스트 예시
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: secure-app
spec:
  template:
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000
      containers:
      - name: app
        image: nginx
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: var-cache
          mountPath: /var/cache/nginx
      volumes:
      - name: tmp
        emptyDir: {}
      - name: var-cache
        emptyDir: {}
EOF
```

## 🔍 모니터링과 로깅

### Q12. Prometheus가 메트릭을 수집하지 않습니다

**A12.** Prometheus 설정 확인:

```bash
# 1. Prometheus Pod 상태 확인
kubectl get pods -n monitoring | grep prometheus

# 2. Prometheus 설정 확인
kubectl get prometheus -n monitoring
kubectl describe prometheus <prometheus-name> -n monitoring

# 3. ServiceMonitor 확인
kubectl get servicemonitor -n monitoring
kubectl describe servicemonitor <sm-name> -n monitoring

# 4. 서비스 라벨 확인
kubectl get svc --show-labels | grep <service-name>

# 5. Prometheus 타겟 확인 (포트 포워딩 후)
kubectl port-forward -n monitoring svc/prometheus-operated 9090:9090
# 브라우저에서 http://localhost:9090/targets 확인

# 6. 애플리케이션 메트릭 엔드포인트 테스트
kubectl exec -it <pod-name> -- curl localhost:<metrics-port>/metrics
```

### Q13. 로그를 중앙에서 수집하려면 어떻게 하나요?

**A13.** EFK 스택 구성:

```bash
# 1. Elasticsearch 설치
helm repo add elastic https://helm.elastic.co
helm install elasticsearch elastic/elasticsearch -n logging --create-namespace

# 2. Fluentd DaemonSet 배포
kubectl apply -f https://raw.githubusercontent.com/fluent/fluentd-kubernetes-daemonset/master/fluentd-daemonset-elasticsearch-rbac.yaml

# 3. Kibana 설치
helm install kibana elastic/kibana -n logging

# 4. 로그 확인
kubectl logs -n logging <fluentd-pod>

# 5. Kibana 접속
kubectl port-forward -n logging svc/kibana-kibana 5601:5601
```

## 💾 스토리지

### Q14. PVC가 Pending 상태입니다

**A14.** 스토리지 문제 해결:

```bash
# 1. StorageClass 확인
kubectl get storageclass
kubectl describe storageclass <sc-name>

# 2. PVC 상세 정보
kubectl describe pvc <pvc-name>

# 3. 기본 StorageClass 설정
kubectl patch storageclass <sc-name> -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

# 4. 수동 PV 생성 (정적 프로비저닝)
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  name: manual-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /tmp/data
EOF
```

### Q15. 컨테이너에서 스토리지에 쓸 수 없습니다

**A15.** 권한 문제 해결:

```bash
# 1. 마운트 포인트 확인
kubectl exec -it <pod-name> -- ls -la /mnt/data

# 2. fsGroup 설정
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-storage
spec:
  securityContext:
    fsGroup: 2000
  containers:
  - name: app
    image: busybox
    command: ["sleep", "3600"]
    volumeMounts:
    - name: data
      mountPath: /mnt/data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: <pvc-name>
EOF

# 3. initContainer로 권한 설정
initContainers:
- name: fix-permissions
  image: busybox
  command: ["chown", "-R", "1000:1000", "/mnt/data"]
  volumeMounts:
  - name: data
    mountPath: /mnt/data
```

## 🌐 네트워킹

### Q16. 외부에서 서비스에 접근할 수 없습니다

**A16.** Ingress와 LoadBalancer 설정:

```bash
# 1. Ingress Controller 확인
kubectl get pods -n ingress-nginx
kubectl get svc -n ingress-nginx

# 2. MetalLB 설정 확인 (베어메탈 환경)
kubectl get pods -n metallb-system
kubectl get ipaddresspool -n metallb-system

# 3. Ingress 리소스 확인
kubectl get ingress
kubectl describe ingress <ingress-name>

# 4. DNS 설정 확인
nslookup <domain-name>

# 5. 방화벽 규칙 확인
sudo iptables -L -n | grep <port>
```

### Q17. Pod 간 통신이 안 됩니다

**A17.** 네트워크 정책 확인:

```bash
# 1. 네트워크 정책 확인
kubectl get networkpolicy --all-namespaces
kubectl describe networkpolicy <policy-name>

# 2. Pod IP 확인
kubectl get pods -o wide

# 3. 연결 테스트
kubectl exec -it <pod1> -- ping <pod2-ip>
kubectl exec -it <pod1> -- telnet <pod2-ip> <port>

# 4. CNI 로그 확인
kubectl logs -n kube-system <cni-pod>

# 5. 네트워크 정책 임시 비활성화 (테스트용)
kubectl delete networkpolicy --all -n <namespace>
```

---

> 💡 **추가 도움말**:
> - 문제 해결 시 항상 `kubectl describe`와 `kubectl logs`를 먼저 확인하세요
> - 시스템 이벤트는 `kubectl get events --sort-by='.lastTimestamp'`로 확인
> - 복잡한 문제는 단계별로 나누어 접근하세요
> - 설정 변경 전에는 반드시 백업을 생성하세요

**관련 문서**: [[09-Operations/트러블슈팅 가이드]] | [[00-Overview/빠른 참조 가이드]] | [[Scripts/클러스터 관리 스크립트]]